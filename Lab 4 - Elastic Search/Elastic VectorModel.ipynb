{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 4: Implementing search in the vector space model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Continue to work with the `arxiv` repository from last session\n",
    "- Learn how to do atomic, conjunctive and disjunctive search with ElasticSearch\n",
    "- Build an inverted index for the `arxiv` repository from last session (should fit in main memory)\n",
    "- Implement search in the vector space model and compare it with ElasticSearch built-in search mechanism\n",
    "- Compare different implementations of search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Built-in search in ElasticSearch\n",
    "\n",
    "ElasticSearch provides a search mechanism to make queries against a database. \n",
    "In the next code snippet you can find examples on how to do this with an atomic query (single term)\n",
    "and with conjunctive and disjunctive queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= -NyUMosBHh_bflI4QOFP SCORE=3.2082987\n",
      "PATH= ../../../arxiv_abs/arxiv\\cs.updates.on.arXiv.org/002772\n",
      "TEXT: Limit computable functions can be characterized by Turing jumps on the input side or limit\n",
      "\n",
      "ID= 0N2UMosBHh_bflI4cDBZ SCORE=3.2082987\n",
      "PATH= ../../../arxiv_abs/arxiv\\math.updates.on.arXiv.org/001904\n",
      "TEXT: Limit computable functions can be characterized by Turing jumps on the input side or limit\n",
      "\n",
      "ID= eNyUMosBHh_bflI4TvNV SCORE=3.1941829\n",
      "PATH= ../../../arxiv_abs/arxiv\\cs.updates.on.arXiv.org/007252\n",
      "TEXT: We study scheduling of computation tasks across $n$ workers in a large scale distributed l\n",
      "\n",
      "ID= bN2UMosBHh_bflI4dDgC SCORE=3.1941829\n",
      "PATH= ../../../arxiv_abs/arxiv\\math.updates.on.arXiv.org/003852\n",
      "TEXT: We study scheduling of computation tasks across $n$ workers in a large scale distributed l\n",
      "\n",
      "ID= htyUMosBHh_bflI4T_TR SCORE=3.1520977\n",
      "PATH= ../../../arxiv_abs/arxiv\\cs.updates.on.arXiv.org/007522\n",
      "TEXT: Grid Computing is an idea of a new kind of network technology in which research work in pr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "## atomic query\n",
    "q = Q('query_string',query='computer')  # Feel free to change the word\n",
    "\n",
    "s = s.query(q)  # add the query to the search object\n",
    "response = s[:5].execute()  # execute the search and return the first 5 results\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print('ID= %s SCORE=%s' % (r.meta.id,  r.meta.score))\n",
    "    print('PATH= %s' % r.path)\n",
    "    print('TEXT: %s' % r.text[:90])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= 5t2UMosBHh_bflI4hWSJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000650\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= Ad2UMosBHh_bflI4hWWJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000677\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= 0N2UMosBHh_bflI4h2gT SCORE=11.029575\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/001652\n",
      "TEXT: A defining feature in the field of quantum computing is the potential of a quantum device \n",
      "\n",
      "ID= vNyUMosBHh_bflI4A55F SCORE=10.839215\n",
      "PATH= ../../../arxiv_abs/arxiv\\astro-ph.updates.on.arXiv.org/006224\n",
      "TEXT: Context. PKS 1510-089 is a flat spectrum radio quasar strongly variable in the optical and\n",
      "\n",
      "ID= etyUMosBHh_bflI4BaEv SCORE=9.807085\n",
      "PATH= ../../../arxiv_abs/arxiv\\astro-ph.updates.on.arXiv.org/006926\n",
      "TEXT: PKS 1510-089 is a flat spectrum radio quasar strongly variable in the optical and GeV rang\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## conjunctive query\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "q = Q('query_string',query='computer') & Q('query_string',query='magic')\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0:5].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print(f'ID= {r.meta.id} SCORE={r.meta.score}')\n",
    "    print(f'PATH= {r.path}')\n",
    "    print(f'TEXT: {r.text[:90]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= 5t2UMosBHh_bflI4hWSJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000650\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= Ad2UMosBHh_bflI4hWWJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000677\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= h9yUMosBHh_bflI4M89P SCORE=12.0883665\n",
      "PATH= ../../../arxiv_abs/arxiv\\cond-mat.updates.on.arXiv.org/003482\n",
      "TEXT: When two monolayers of graphene are stacked with a small relative twist angle, the resulti\n",
      "\n",
      "ID= aN2UMosBHh_bflI4ayQI SCORE=11.981175\n",
      "PATH= ../../../arxiv_abs/arxiv\\hep-th.updates.on.arXiv.org/000265\n",
      "TEXT: We introduce the extended Freudenthal-Rosenfeld-Tits magic square based on six algebras: t\n",
      "\n",
      "ID= G92UMosBHh_bflI4bi3m SCORE=11.981175\n",
      "PATH= ../../../arxiv_abs/arxiv\\math.updates.on.arXiv.org/000955\n",
      "TEXT: We introduce the extended Freudenthal-Rosenfeld-Tits magic square based on six algebras: t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## disjunctive query\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "q = Q('query_string',query='computer') | Q('query_string',query='magic')\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0:5].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print(f'ID= {r.meta.id} SCORE={r.meta.score}')\n",
    "    print(f'PATH= {r.path}')\n",
    "    print(f'TEXT: {r.text[:90]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Excruciatingly slow search\n",
    "\n",
    "In class we have presented a _slow_ version of search that, given a search query $q$, loops over every document in the database\n",
    "computing the cosine similarity between document and query. Once this is done, it sorts documents by their similarity w.r.t. $q$ and returns the top $r$\n",
    "scoring ones. \n",
    "\n",
    "```\n",
    "1. for each d in D:\n",
    "    sim(d,q) = 0\n",
    "    get vector representing d\n",
    "    for each w in q:\n",
    "        sim(d,q) += tf(d,w) * idf(w)\n",
    "    normalize sim(d,q) by |d|*|q|\n",
    "2. sort results by similarity\n",
    "3. return top r docs\n",
    "```\n",
    "\n",
    "A possible implementation can be found below. \n",
    "\n",
    "__Remark:__ _It should be important to note that there are certain elements in the implementation below that refer to my own\n",
    "implementation, and that you should adapt to your own; in particular, the line_\n",
    "\n",
    "```    weights = dict(normalize(tf_idf(s['_id'])))   # gets weights as a python dict of term -> weight ```\n",
    "\n",
    "_obtains tf-idf weights through calling a function `tf_idf` that I have implemented that, given a docid, returns a list of pairs (term, weight); and `normalize` takes such a list a normalizes weights so that the corresponding vector has length 1. \n",
    "Obviously, you should adapt the code to your own implementations from previous sessions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "import tqdm\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf-idf list of pairs (term, weight) from doc (internal) id\n",
    "def tf_idf(idx: str, client, doc_id: str, D: int) -> list:\n",
    "    \"\"\"\n",
    "    Compute tf-idf for each term in a document with internal id doc_id\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    tv = client.termvectors(index=idx, id=doc_id, fields=['text'], term_statistics=True)\n",
    "\n",
    "    tfidf = []\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        max_word = max(tv['term_vectors']['text']['terms'], key=lambda x: tv['term_vectors']['text']['terms'][x]['term_freq'])\n",
    "        max_freq = tv['term_vectors']['text']['terms'][max_word]['term_freq']\n",
    "        \n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            word = t\n",
    "            \n",
    "            word_freq = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            df = tv['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "\n",
    "            tfidf.append((word, (word_freq/max_freq) * math.log(D/df, 2)))\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizes weights so that resulting vec has length 1\n",
    "def normalize(tdfidf: list) -> list:\n",
    "    \"\"\"\n",
    "    Normalize tf-idf weights so that the resulting vector has length 1\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    norm_d = math.sqrt(sum([w**2 for _, w in tdfidf]))\n",
    "    return [(t, w/norm_d) for t, w in tdfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/58102 [00:00<2:04:32,  7.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [10:26<00:00, 92.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('../../../arxiv_abs/arxiv\\\\quant-ph.updates.on.arXiv.org/000650',\n",
      "  0.47752597051076895),\n",
      " ('../../../arxiv_abs/arxiv\\\\quant-ph.updates.on.arXiv.org/000677',\n",
      "  0.477263052236843),\n",
      " ('../../../arxiv_abs/arxiv\\\\cond-mat.updates.on.arXiv.org/003482',\n",
      "  0.38792349570755896),\n",
      " ('../../../arxiv_abs/arxiv\\\\quant-ph.updates.on.arXiv.org/001475',\n",
      "  0.3023262889426185),\n",
      " ('../../../arxiv_abs/arxiv\\\\hep-th.updates.on.arXiv.org/000265',\n",
      "  0.26483163493536094),\n",
      " ('../../../arxiv_abs/arxiv\\\\math.updates.on.arXiv.org/000955',\n",
      "  0.26483163493536094),\n",
      " ('../../../arxiv_abs/arxiv\\\\hep-th.updates.on.arXiv.org/000255',\n",
      "  0.24832299488605178),\n",
      " ('../../../arxiv_abs/arxiv\\\\math.updates.on.arXiv.org/000896',\n",
      "  0.24832299488605178),\n",
      " ('../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/002083',\n",
      "  0.24062631526975578),\n",
      " ('../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/001135',\n",
      "  0.23778079125705068)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "r = 10  # only return r top docs\n",
    "query = 'computer magic'\n",
    "sims = dict()\n",
    "\n",
    "l2query  = np.sqrt(len(query.split()))  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "# get nr. of docs; just for the progress bar\n",
    "ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])  # D\n",
    "\n",
    "# scan through docs, compute cosine sim between query and each doc\n",
    "for s in tqdm.tqdm(scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "    docid = s['_source']['path']   # use path as id\n",
    "\n",
    "    sims[docid] = 0.0\n",
    "    weights = dict(normalize(tf_idf('arxiv', client, s['_id'], ndocs)))  # normalize weights for doc\n",
    "\n",
    "    for w in query.split():  # gets terms as a list\n",
    "        if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "            sims[docid] += weights[w]   # accumulates if w in current doc\n",
    "\n",
    "    # normalize sim\n",
    "    sims[docid] /= l2query  # ||q||_2 = 1\n",
    "\n",
    "# now sort by cosine similarity\n",
    "sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "pprint(sorted_answer[:r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch.helpers import scan\n",
    "# from pprint import pprint\n",
    "# from elasticsearch import Elasticsearch\n",
    "# import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# # get tf-idf vector from doc (internal) id\n",
    "# def tf_idf(doc_id):\n",
    "#     # does nothing, adapt to your needs\n",
    "#     return []\n",
    "\n",
    "# # normalizes weights so that resulting vec has length 1\n",
    "# def normalize(l1):\n",
    "#     # does nothing, adapt to your needs\n",
    "#     return l1\n",
    "\n",
    "# client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "# r = 10  # only return r top docs\n",
    "# query = 'computer magic'\n",
    "# sims = dict()\n",
    "\n",
    "# l2query  = np.sqrt(len(query.split()))  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "# # get nr. of docs; just for the progress bar\n",
    "# ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])\n",
    "\n",
    "# # scan through docs, compute cosine sim between query and each doc\n",
    "# for s in tqdm.tqdm(scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "#     docid = s['_source']['path']   # use path as id\n",
    "    \n",
    "#     sims[docid] = 0.0\n",
    "#     weights = dict(normalize(tf_idf(s['_id'])))   # get tf-idf weights representing doc as dict\n",
    "#     for w in query.split():  # gets terms as a list\n",
    "#         if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "#             sims[docid] += weights[w]   # accumulates if w in current doc\n",
    "#     # normalize sim\n",
    "#     sims[docid] /= l2query\n",
    "\n",
    "# # now sort by cosine similarity\n",
    "# sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "# pprint(sorted_answer[:r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140 docs with non-zero similarity out of 58102, i.e. 0.2%\n"
     ]
    }
   ],
   "source": [
    "nz = len([x for x, s in sorted_answer if s>0])\n",
    "total = len(sorted_answer)\n",
    "print(f'There are {nz} docs with non-zero similarity out of {total}, i.e. {100.0*nz/total:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your tasks\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1:**  \n",
    "\n",
    "Make sure you understand the algorithm for implementing search described in the lecture notes. Both slow and efficient versions. Describe\n",
    "the number of sums you need to do in both slow and quick versions for the following toy example with a vocabulary of size 4 and four documents:\n",
    "\n",
    "- $q = 0,1,1,0$\n",
    "\n",
    "- document-term matrix:\n",
    "<center>\n",
    "\n",
    "\n",
    "|        | t1  | t2  | t3  | t4  |\n",
    "|--------|-----|-----|-----|-----|\n",
    "| **d1** | 1.2 | 0.0 | 0.0 | 0.0 |\n",
    "| **d2** | 0.7 | 0.3 | 1.5 | 0.1 |\n",
    "| **d3** | 0.0 | 0.0 | 0.0 | 0.7 |\n",
    "| **d4** | 2.0 | 0.0 | 0.0 | 0.0 |\n",
    "\n",
    "</center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 2:**\n",
    "\n",
    "Implement the quick version; run both slow and quick versions and report times (as a reference, in my old laptop it takes around 5m30s to run the slow version in the code above). Make sure both versions return the same answer. Note that you will need to build an inverted index in order to implement the efficient version as explained in class; it may take time but this is done once for all queries, and can be done \"off-line\".\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Carlos Arbonés\\Desktop\\repositories\\Search-and-Analysis-of-Information\\Lab 4 - Elastic Search\\Elastic VectorModel.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%204%20-%20Elastic%20Search/Elastic%20VectorModel.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m posting_list \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%204%20-%20Elastic%20Search/Elastic%20VectorModel.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(scan(client, index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marxiv\u001b[39m\u001b[39m'\u001b[39m, query\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m : {\u001b[39m\"\u001b[39m\u001b[39mmatch_all\u001b[39m\u001b[39m\"\u001b[39m: {}}}), total\u001b[39m=\u001b[39mndocs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%204%20-%20Elastic%20Search/Elastic%20VectorModel.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   docid \u001b[39m=\u001b[39m s[\u001b[39m'\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%204%20-%20Elastic%20Search/Elastic%20VectorModel.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   tv \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mtermvectors(index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marxiv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39mdocid, fields\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], term_statistics\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "posting_list = dict()\n",
    "\n",
    "for s in tqdm.tqdm(scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "  docid = s['_id']\n",
    "  \n",
    "  tv = client.termvectors(index='arxiv', id=docid, fields=['text'], term_statistics=True)\n",
    "  \n",
    "  if 'text' in tv['term_vectors']:\n",
    "    for t in tv['term_vectors']['text']['terms']:\n",
    "      if t not in posting_list:\n",
    "        posting_list[t] = set()\n",
    "      posting_list[t].add(docid)\n",
    "      \n",
    "pprint(posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5t2UMosBHh_bflI4hWSJ', 0.47752597051076895),\n",
       " ('Ad2UMosBHh_bflI4hWWJ', 0.477263052236843),\n",
       " ('h9yUMosBHh_bflI4M89P', 0.38792349570755896),\n",
       " ('H92UMosBHh_bflI4h2gT', 0.3023262889426185),\n",
       " ('aN2UMosBHh_bflI4ayQI', 0.26483163493536094),\n",
       " ('G92UMosBHh_bflI4bi3m', 0.26483163493536094),\n",
       " ('4N2UMosBHh_bflI4bizm', 0.24832299488605178),\n",
       " ('Xt2UMosBHh_bflI4ayQI', 0.24832299488605178),\n",
       " ('j9yTMosBHh_bflI48I7S', 0.24062631526975578),\n",
       " ('29yTMosBHh_bflI47Ioz', 0.23778079125705068)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverted_file_implementation(query: str,  \n",
    "                                 client: Elasticsearch,\n",
    "                                 posting_list: dict, \n",
    "                                 ndocs:int, \n",
    "                                 r: int) -> list:\n",
    "  \"\"\"\n",
    "  Implement inverted file retrieval for a query and return top r results\n",
    "  \"\"\"\n",
    "  sims = dict()\n",
    "  \n",
    "  for w in query.split(\" \"):\n",
    "    L = posting_list[w]\n",
    "    for d in L:\n",
    "      weights = dict(normalize(tf_idf('arxiv', client, d, ndocs)))\n",
    "      if d not in sims:\n",
    "        sims[d] = 0.0 \n",
    "      sims[d] += weights[w]\n",
    "  \n",
    "  l2query = np.sqrt(len(query.split()))  \n",
    "  for d in sims:\n",
    "    sims[d] /= l2query\n",
    "    \n",
    "  sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "  return sorted_answer[:r]\n",
    "\n",
    "query = 'computer magic'\n",
    "r = 10\n",
    "idx = 'arxiv'\n",
    "ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])\n",
    "\n",
    "inverted_file_implementation(query, client, posting_list, ndocs, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Compare the results for a few sample queries that you get from your quick version and ElasticSearch search. Do you get similar results? Which is faster?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rules of delivery\n",
    "\n",
    "- To be solved in _pairs_.\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session.\n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report listing the solutions to the exercises proposed. Include things like the important parts of your implementation (data structures used for representing objects, algorithms used, etc). You are welcome to add conclusions and findings that depart from what we asked you to do. We encourage you to discuss the difficulties you find; this lets us give you help and also improve the lab session for future editions.\n",
    "\n",
    "- Turn the report to PDF. Make sure it has your names, date, and title. Include your code in your submission.\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html) _before November 6th, 2023_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
