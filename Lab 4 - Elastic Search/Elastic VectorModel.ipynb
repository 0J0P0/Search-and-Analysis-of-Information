{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 4: Implementing search in the vector space model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Continue to work with the `arxiv` repository from last session\n",
    "- Learn how to do atomic, conjunctive and disjunctive search with ElasticSearch\n",
    "- Build an inverted index for the `arxiv` repository from last session (should fit in main memory)\n",
    "- Implement search in the vector space model and compare it with ElasticSearch built-in search mechanism\n",
    "- Compare different implementations of search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Built-in search in ElasticSearch\n",
    "\n",
    "ElasticSearch provides a search mechanism to make queries against a database. \n",
    "In the next code snippet you can find examples on how to do this with an atomic query (single term)\n",
    "and with conjunctive and disjunctive queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= lpEAV4sBcL-B_pFl6BAW SCORE=3.2082987\n",
      "PATH= ../../arxiv\\cs.updates.on.arXiv.org/002772\n",
      "TEXT: Limit computable functions can be characterized by Turing jumps on the input side or limit\n",
      "\n",
      "ID= bpEAV4sBcL-B_pFl-19e SCORE=3.2082987\n",
      "PATH= ../../arxiv\\math.updates.on.arXiv.org/001904\n",
      "TEXT: Limit computable functions can be characterized by Turing jumps on the input side or limit\n",
      "\n",
      "ID= FpEAV4sBcL-B_pFl7CJ2 SCORE=3.1941829\n",
      "PATH= ../../arxiv\\cs.updates.on.arXiv.org/007252\n",
      "TEXT: We study scheduling of computation tasks across $n$ workers in a large scale distributed l\n",
      "\n",
      "ID= CpEAV4sBcL-B_pFl_Wcq SCORE=3.1941829\n",
      "PATH= ../../arxiv\\math.updates.on.arXiv.org/003852\n",
      "TEXT: We study scheduling of computation tasks across $n$ workers in a large scale distributed l\n",
      "\n",
      "ID= JJEAV4sBcL-B_pFl7SMA SCORE=3.1520977\n",
      "PATH= ../../arxiv\\cs.updates.on.arXiv.org/007522\n",
      "TEXT: Grid Computing is an idea of a new kind of network technology in which research work in pr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "## atomic query\n",
    "q = Q('query_string',query='computer')  # Feel free to change the word\n",
    "\n",
    "s = s.query(q)  # add the query to the search object\n",
    "response = s[:5].execute()  # execute the search and return the first 5 results\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print('ID= %s SCORE=%s' % (r.meta.id,  r.meta.score))\n",
    "    print('PATH= %s' % r.path)\n",
    "    print('TEXT: %s' % r.text[:90])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= hJEBV4sBcL-B_pFlB5Oa SCORE=14.512409\n",
      "PATH= ../../arxiv\\quant-ph.updates.on.arXiv.org/000650\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= n5EBV4sBcL-B_pFlB5Oa SCORE=14.512409\n",
      "PATH= ../../arxiv\\quant-ph.updates.on.arXiv.org/000677\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= bpEBV4sBcL-B_pFlCJeR SCORE=11.029575\n",
      "PATH= ../../arxiv\\quant-ph.updates.on.arXiv.org/001652\n",
      "TEXT: A defining feature in the field of quantum computing is the potential of a quantum device \n",
      "\n",
      "ID= WpAAV4sBcL-B_pFl2M1W SCORE=10.839215\n",
      "PATH= ../../arxiv\\astro-ph.updates.on.arXiv.org/006224\n",
      "TEXT: Context. PKS 1510-089 is a flat spectrum radio quasar strongly variable in the optical and\n",
      "\n",
      "ID= GJAAV4sBcL-B_pFl2NDh SCORE=9.807085\n",
      "PATH= ../../arxiv\\astro-ph.updates.on.arXiv.org/006926\n",
      "TEXT: PKS 1510-089 is a flat spectrum radio quasar strongly variable in the optical and GeV rang\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## conjunctive query\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "q = Q('query_string',query='computer') & Q('query_string',query='magic')\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0:5].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print(f'ID= {r.meta.id} SCORE={r.meta.score}')\n",
    "    print(f'PATH= {r.path}')\n",
    "    print(f'TEXT: {r.text[:90]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= hJEBV4sBcL-B_pFlB5Oa SCORE=14.512409\n",
      "PATH= ../../arxiv\\quant-ph.updates.on.arXiv.org/000650\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= n5EBV4sBcL-B_pFlB5Oa SCORE=14.512409\n",
      "PATH= ../../arxiv\\quant-ph.updates.on.arXiv.org/000677\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= JZAAV4sBcL-B_pFl4_72 SCORE=12.0883665\n",
      "PATH= ../../arxiv\\cond-mat.updates.on.arXiv.org/003482\n",
      "TEXT: When two monolayers of graphene are stacked with a small relative twist angle, the resulti\n",
      "\n",
      "ID= BpEAV4sBcL-B_pFl-FNF SCORE=11.981175\n",
      "PATH= ../../arxiv\\hep-th.updates.on.arXiv.org/000265\n",
      "TEXT: We introduce the extended Freudenthal-Rosenfeld-Tits magic square based on six algebras: t\n",
      "\n",
      "ID= uZEAV4sBcL-B_pFl-luA SCORE=11.981175\n",
      "PATH= ../../arxiv\\math.updates.on.arXiv.org/000955\n",
      "TEXT: We introduce the extended Freudenthal-Rosenfeld-Tits magic square based on six algebras: t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## disjunctive query\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "q = Q('query_string',query='computer') | Q('query_string',query='magic')\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0:5].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print(f'ID= {r.meta.id} SCORE={r.meta.score}')\n",
    "    print(f'PATH= {r.path}')\n",
    "    print(f'TEXT: {r.text[:90]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Excruciatingly slow search\n",
    "\n",
    "In class we have presented a _slow_ version of search that, given a search query $q$, loops over every document in the database\n",
    "computing the cosine similarity between document and query. Once this is done, it sorts documents by their similarity w.r.t. $q$ and returns the top $r$\n",
    "scoring ones. \n",
    "\n",
    "```\n",
    "1. for each d in D:\n",
    "    sim(d,q) = 0\n",
    "    get vector representing d\n",
    "    for each w in q:\n",
    "        sim(d,q) += tf(d,w) * idf(w)\n",
    "    normalize sim(d,q) by |d|*|q|\n",
    "2. sort results by similarity\n",
    "3. return top r docs\n",
    "```\n",
    "\n",
    "A possible implementation can be found below. \n",
    "\n",
    "__Remark:__ _It should be important to note that there are certain elements in the implementation below that refer to my own\n",
    "implementation, and that you should adapt to your own; in particular, the line_\n",
    "\n",
    "```    weights = dict(normalize(tf_idf(s['_id'])))   # gets weights as a python dict of term -> weight ```\n",
    "\n",
    "_obtains tf-idf weights through calling a function `tf_idf` that I have implemented that, given a docid, returns a list of pairs (term, weight); and `normalize` takes such a list a normalizes weights so that the corresponding vector has length 1. \n",
    "Obviously, you should adapt the code to your own implementations from previous sessions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "import tqdm\n",
    "from colorama import Fore\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(idx: str, client: Elasticsearch, doc_id: str, D: int) -> list:\n",
    "    \"\"\"\n",
    "    Compute tf-idf for each term in a document with internal id doc_id\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    tv = client.termvectors(index=idx, id=doc_id, fields=['text'], term_statistics=True)\n",
    "    tfidf = []\n",
    "    \n",
    "    if 'text' in tv['term_vectors']:\n",
    "        max_word = max(tv['term_vectors']['text']['terms'], key=lambda x: tv['term_vectors']['text']['terms'][x]['term_freq'])\n",
    "        max_fdj = tv['term_vectors']['text']['terms'][max_word]['term_freq']\n",
    "        \n",
    "        for word in tv['term_vectors']['text']['terms']:\n",
    "        \n",
    "            fdi = tv['term_vectors']['text']['terms'][word]['term_freq']    # term frequency in document\n",
    "            dfi = tv['term_vectors']['text']['terms'][word]['doc_freq']     # number of documents containing term in entire corpus\n",
    "            \n",
    "            tf = fdi/max_fdj\n",
    "            idf = math.log(D/dfi, 2)\n",
    "            \n",
    "            tfidf.append((word, tf*idf))\n",
    "            \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tdfidf: list) -> list:\n",
    "    \"\"\"\n",
    "    Normalize tf-idf weights so that the resulting vector has length 1\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    norm_d = math.sqrt(sum([w**2 for _, w in tdfidf]))\n",
    "    return [(t, w/norm_d) for t, w in tdfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "r = 10  # only return r top docs\n",
    "query = 'computer magic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_search(query: str, r: int) -> dict:\n",
    "    \"\"\"\n",
    "    Slow search using tf-idf\n",
    "    \"\"\"\n",
    "\n",
    "    sims = dict()\n",
    "\n",
    "    l2query  = np.sqrt(len(query.split()))  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "    # get nr. of docs; just for the progress bar\n",
    "    ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])  # D\n",
    "\n",
    "    # scan through docs, compute cosine sim between query and each doc\n",
    "    for s in tqdm.tqdm(scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "        docid = s['_id']   # use path as id\n",
    "\n",
    "        sims[docid] = 0.0\n",
    "        weights = dict(normalize(tf_idf('arxiv', client, s['_id'], ndocs)))  # normalize weights for doc\n",
    "\n",
    "        for w in query.split():  # gets terms as a list\n",
    "            if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "                sims[docid] += weights[w]   # accumulates if w in current doc\n",
    "\n",
    "        # normalize sim\n",
    "        sims[docid] /= l2query  # ||q||_2 = 1\n",
    "\n",
    "    # now sort by cosine similarity\n",
    "    sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    return sorted_answer, sorted_answer[:r], len(sorted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [04:37<00:00, 209.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hJEBV4sBcL-B_pFlB5Oa', 0.47752597051076895),\n",
      " ('n5EBV4sBcL-B_pFlB5Oa', 0.477263052236843),\n",
      " ('JZAAV4sBcL-B_pFl4_72', 0.38792349570755896),\n",
      " ('vZEBV4sBcL-B_pFlCJaR', 0.3023262889426185),\n",
      " ('BpEAV4sBcL-B_pFl-FNF', 0.26483163493536094),\n",
      " ('uZEAV4sBcL-B_pFl-luA', 0.26483163493536094),\n",
      " ('_JEAV4sBcL-B_pFl-FJF', 0.24832299488605178),\n",
      " ('fpEAV4sBcL-B_pFl-luA', 0.24832299488605178),\n",
      " ('LZAAV4sBcL-B_pFl1L09', 0.24062631526975578),\n",
      " ('eZAAV4sBcL-B_pFl07k6', 0.23778079125705068)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "complete_answer, answer, total = slow_search(query, r)\n",
    "pprint(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140 docs with non-zero similarity out of 58102, i.e. 0.2%\n"
     ]
    }
   ],
   "source": [
    "nz = len([x for x, s in complete_answer if s>0])\n",
    "print(f'There are {nz} docs with non-zero similarity out of {total}, i.e. {100.0*nz/total:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your tasks\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1:**  \n",
    "\n",
    "Make sure you understand the algorithm for implementing search described in the lecture notes. Both slow and efficient versions. Describe\n",
    "the number of sums you need to do in both slow and quick versions for the following toy example with a vocabulary of size 4 and four documents:\n",
    "\n",
    "- $q = 0,1,1,0$\n",
    "\n",
    "- document-term matrix:\n",
    "<center>\n",
    "\n",
    "\n",
    "|        | t1  | t2  | t3  | t4  |\n",
    "|--------|-----|-----|-----|-----|\n",
    "| **d1** | 1.2 | 0.0 | 0.0 | 0.0 |\n",
    "| **d2** | 0.7 | 0.3 | 1.5 | 0.1 |\n",
    "| **d3** | 0.0 | 0.0 | 0.0 | 0.7 |\n",
    "| **d4** | 2.0 | 0.0 | 0.0 | 0.0 |\n",
    "\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "If we use the slow (inefficient) algorithm to implement the vectorial model, we will need to perform a total of 8 additions. In this version, we traverse the corpus document by document, and for each document, we need to calculate the similarity with the corresponding word from the query q. In this case, with 4 documents and the query containing 2 words (t2, t3), for each document, we have to perform 2 additions,  which totals 4x2 = 8 additions.\n",
    "\n",
    "On the other hand, in the fast algorithm, where we use the inverted file, the total number of additions we need to make is 2. This is because we now traverse the weight matrix by columns/terms instead of rows/documents. For each word in the query, we extract its posting list (a list of documents that contain it), and for these documents, we calculate the partial similarity of the query word with the word in each respective document. Thus, instead of having the entire similarity for some documents at a single point $i$ in the execution, we have a partial similarity for all documents. In this case, we would first take the word t2, and since it only appears in document d2, we would perform 1 addition. Next, we would do the same for t3, which again only appears in d2, resulting in one more addition, making a total of 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 2:**\n",
    "\n",
    "Implement the quick version; run both slow and quick versions and report times (as a reference, in my old laptop it takes around 5m30s to run the slow version in the code above). Make sure both versions return the same answer. Note that you will need to build an inverted index in order to implement the efficient version as explained in class; it may take time but this is done once for all queries, and can be done \"off-line\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(client: Elasticsearch, idx: str) -> dict:\n",
    "  \"\"\"\n",
    "  Returns the inverted index as a dictionary. The inverted index is a dictionary mapping terms to the set of documents that contain the term\n",
    "\n",
    "  ----------\n",
    "  Parameters\n",
    "  ----------\n",
    "  client: Elasticsearch\n",
    "      Elasticsearch client object\n",
    "  idx: str\n",
    "      Name of the index\n",
    "  -------\n",
    "  Returns\n",
    "  -------\n",
    "  dict\n",
    "      Inverted index\n",
    "  \"\"\"\n",
    "\n",
    "  D = int(client.cat.count(index=idx, format = \"json\")[0]['count'])\n",
    "  posting_list = dict()\n",
    "  \n",
    "  print(f'There are {D} documents in the index. Start of the posting list construction...')\n",
    "  for s in tqdm.tqdm(scan(client, index=idx, query={\"query\" : {\"match_all\": {}}}), total=D):\n",
    "    docid = s['_id']\n",
    "    tv = client.termvectors(index=idx, id=docid, fields=['text'], term_statistics=True)\n",
    "    \n",
    "    if 'text' in tv['term_vectors']:\n",
    "      for t in tv['term_vectors']['text']['terms']:\n",
    "        if t not in posting_list:\n",
    "          posting_list[t] = set()\n",
    "        posting_list[t].add(docid)\n",
    "  print(Fore.GREEN + 'Posting list construction completed.')\n",
    "  return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_search(query: str,  \n",
    "                                 client: Elasticsearch,\n",
    "                                 posting_list: dict, \n",
    "                                 D:int, \n",
    "                                 r: int) -> list:\n",
    "  \"\"\"\n",
    "  Implement inverted file retrieval for a query and return top r results\n",
    "\n",
    "  ----------\n",
    "  Parameters\n",
    "  ----------\n",
    "  query: str\n",
    "      Query string\n",
    "  client: Elasticsearch\n",
    "      Elasticsearch client object\n",
    "  posting_list: dict\n",
    "      Inverted index\n",
    "  D: int\n",
    "      Number of documents in the collection\n",
    "  r: int\n",
    "      Number of results to return\n",
    "  -------\n",
    "  Returns\n",
    "  -------\n",
    "  list\n",
    "      Top r results\n",
    "  \"\"\"\n",
    "  sims = dict()\n",
    "  \n",
    "  for w in tqdm.tqdm(query.split()):\n",
    "    L = posting_list[w]\n",
    "    for d in L:\n",
    "      weights = dict(normalize(tf_idf('arxiv', client, d, D)))\n",
    "      if d not in sims:\n",
    "        sims[d] = 0.0 \n",
    "      sims[d] += weights[w]\n",
    "  \n",
    "  l2query = np.sqrt(len(query.split()))  \n",
    "  for d in sims:\n",
    "    sims[d] /= l2query\n",
    "    \n",
    "  sorted_by_similarity = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "  return sorted_by_similarity[:r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58102 documents in the index. Start of the posting list construction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [04:34<00:00, 211.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mPosting list construction completed.\n"
     ]
    }
   ],
   "source": [
    "pst_list = inverted_index(client, 'arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hJEBV4sBcL-B_pFlB5Oa', 0.47752597051076895),\n",
       " ('n5EBV4sBcL-B_pFlB5Oa', 0.477263052236843),\n",
       " ('JZAAV4sBcL-B_pFl4_72', 0.38792349570755896),\n",
       " ('vZEBV4sBcL-B_pFlCJaR', 0.3023262889426185),\n",
       " ('uZEAV4sBcL-B_pFl-luA', 0.26483163493536094),\n",
       " ('BpEAV4sBcL-B_pFl-FNF', 0.26483163493536094),\n",
       " ('_JEAV4sBcL-B_pFl-FJF', 0.24832299488605178),\n",
       " ('fpEAV4sBcL-B_pFl-luA', 0.24832299488605178),\n",
       " ('LZAAV4sBcL-B_pFl1L09', 0.24062631526975578),\n",
       " ('eZAAV4sBcL-B_pFl07k6', 0.23778079125705068)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 10\n",
    "query = 'computer magic'\n",
    "D = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])\n",
    "inverted_index_search(query, client, pst_list, D, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Compare the results for a few sample queries that you get from your quick version and ElasticSearch search. Do you get similar results? Which is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_query_execution_times(query: str, \n",
    "                                  client: Elasticsearch, \n",
    "                                  posting_list: dict, \n",
    "                                  ndocs:int,\n",
    "                                  r: int) -> list:\n",
    "  \"\"\"\n",
    "  Compare query execution times for the two implementations\n",
    "  \"\"\"\n",
    "  import time\n",
    "  \n",
    "  start = time.time()\n",
    "  Own_implementation = inverted_index_search(query, client, posting_list, ndocs, r)\n",
    "  end = time.time()\n",
    "  print(f'Inverted file implementation took {end-start:.2f} seconds')\n",
    "  \n",
    "  start = time.time()\n",
    "  s = Search(using=client, index='arxiv')\n",
    "  q = Q('query_string', query=query)\n",
    "  s = s.query(q)\n",
    "  Elasticsearch_implementation = s[:r].execute()\n",
    "  end = time.time()\n",
    "  print(f'Elasticsearch implementation took {end-start:.2f} seconds \\n')\n",
    "\n",
    "  print(f'Let''s compare the results of the two implementations: \\n')\n",
    "  for i in range(r): \n",
    "    print(f'Inverted file implementation: {Own_implementation[i][0]} with score {Own_implementation[i][1]}')\n",
    "    print(f'Elasticsearch implementation: {Elasticsearch_implementation[i].meta.id} with score {Elasticsearch_implementation[i].meta.score} \\n')\n",
    "\n",
    "\n",
    "query = 'computer magic'\n",
    "compare_query_execution_times(query, client, pst_list, D, r)\n",
    "\n",
    "# english_words = words.words()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rules of delivery\n",
    "\n",
    "- To be solved in _pairs_.\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session.\n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report listing the solutions to the exercises proposed. Include things like the important parts of your implementation (data structures used for representing objects, algorithms used, etc). You are welcome to add conclusions and findings that depart from what we asked you to do. We encourage you to discuss the difficulties you find; this lets us give you help and also improve the lab session for future editions.\n",
    "\n",
    "- Turn the report to PDF. Make sure it has your names, date, and title. Include your code in your submission.\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html) _before November 6th, 2023_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
