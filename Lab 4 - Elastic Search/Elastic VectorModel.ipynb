{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 4: Implementing search in the vector space model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Continue to work with the `arxiv` repository from last session\n",
    "- Learn how to do atomic, conjunctive and disjunctive search with ElasticSearch\n",
    "- Build an inverted index for the `arxiv` repository from last session (should fit in main memory)\n",
    "- Implement search in the vector space model and compare it with ElasticSearch built-in search mechanism\n",
    "- Compare different implementations of search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Built-in search in ElasticSearch\n",
    "\n",
    "ElasticSearch provides a search mechanism to make queries against a database. \n",
    "In the next code snippet you can find examples on how to do this with an atomic query (single term)\n",
    "and with conjunctive and disjunctive queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= -NyUMosBHh_bflI4QOFP SCORE=3.2082987\n",
      "PATH= ../../../arxiv_abs/arxiv\\cs.updates.on.arXiv.org/002772\n",
      "TEXT: Limit computable functions can be characterized by Turing jumps on the input side or limit\n",
      "\n",
      "ID= 0N2UMosBHh_bflI4cDBZ SCORE=3.2082987\n",
      "PATH= ../../../arxiv_abs/arxiv\\math.updates.on.arXiv.org/001904\n",
      "TEXT: Limit computable functions can be characterized by Turing jumps on the input side or limit\n",
      "\n",
      "ID= eNyUMosBHh_bflI4TvNV SCORE=3.1941829\n",
      "PATH= ../../../arxiv_abs/arxiv\\cs.updates.on.arXiv.org/007252\n",
      "TEXT: We study scheduling of computation tasks across $n$ workers in a large scale distributed l\n",
      "\n",
      "ID= bN2UMosBHh_bflI4dDgC SCORE=3.1941829\n",
      "PATH= ../../../arxiv_abs/arxiv\\math.updates.on.arXiv.org/003852\n",
      "TEXT: We study scheduling of computation tasks across $n$ workers in a large scale distributed l\n",
      "\n",
      "ID= htyUMosBHh_bflI4T_TR SCORE=3.1520977\n",
      "PATH= ../../../arxiv_abs/arxiv\\cs.updates.on.arXiv.org/007522\n",
      "TEXT: Grid Computing is an idea of a new kind of network technology in which research work in pr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "## atomic query\n",
    "q = Q('query_string',query='computer')  # Feel free to change the word\n",
    "\n",
    "s = s.query(q)  # add the query to the search object\n",
    "response = s[:5].execute()  # execute the search and return the first 5 results\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print('ID= %s SCORE=%s' % (r.meta.id,  r.meta.score))\n",
    "    print('PATH= %s' % r.path)\n",
    "    print('TEXT: %s' % r.text[:90])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= 5t2UMosBHh_bflI4hWSJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000650\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= Ad2UMosBHh_bflI4hWWJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000677\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= 0N2UMosBHh_bflI4h2gT SCORE=11.029575\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/001652\n",
      "TEXT: A defining feature in the field of quantum computing is the potential of a quantum device \n",
      "\n",
      "ID= vNyUMosBHh_bflI4A55F SCORE=10.839215\n",
      "PATH= ../../../arxiv_abs/arxiv\\astro-ph.updates.on.arXiv.org/006224\n",
      "TEXT: Context. PKS 1510-089 is a flat spectrum radio quasar strongly variable in the optical and\n",
      "\n",
      "ID= etyUMosBHh_bflI4BaEv SCORE=9.807085\n",
      "PATH= ../../../arxiv_abs/arxiv\\astro-ph.updates.on.arXiv.org/006926\n",
      "TEXT: PKS 1510-089 is a flat spectrum radio quasar strongly variable in the optical and GeV rang\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## conjunctive query\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "q = Q('query_string',query='computer') & Q('query_string',query='magic')\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0:5].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print(f'ID= {r.meta.id} SCORE={r.meta.score}')\n",
    "    print(f'PATH= {r.path}')\n",
    "    print(f'TEXT: {r.text[:90]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID= 5t2UMosBHh_bflI4hWSJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000650\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= Ad2UMosBHh_bflI4hWWJ SCORE=14.512409\n",
      "PATH= ../../../arxiv_abs/arxiv\\quant-ph.updates.on.arXiv.org/000677\n",
      "TEXT: We give a new algorithm for computing the robustness of magic - a measure of the utility o\n",
      "\n",
      "ID= h9yUMosBHh_bflI4M89P SCORE=12.0883665\n",
      "PATH= ../../../arxiv_abs/arxiv\\cond-mat.updates.on.arXiv.org/003482\n",
      "TEXT: When two monolayers of graphene are stacked with a small relative twist angle, the resulti\n",
      "\n",
      "ID= aN2UMosBHh_bflI4ayQI SCORE=11.981175\n",
      "PATH= ../../../arxiv_abs/arxiv\\hep-th.updates.on.arXiv.org/000265\n",
      "TEXT: We introduce the extended Freudenthal-Rosenfeld-Tits magic square based on six algebras: t\n",
      "\n",
      "ID= G92UMosBHh_bflI4bi3m SCORE=11.981175\n",
      "PATH= ../../../arxiv_abs/arxiv\\math.updates.on.arXiv.org/000955\n",
      "TEXT: We introduce the extended Freudenthal-Rosenfeld-Tits magic square based on six algebras: t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## disjunctive query\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "s = Search(using=client, index='arxiv')\n",
    "\n",
    "q = Q('query_string',query='computer') | Q('query_string',query='magic')\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0:5].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print(f'ID= {r.meta.id} SCORE={r.meta.score}')\n",
    "    print(f'PATH= {r.path}')\n",
    "    print(f'TEXT: {r.text[:90]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Excruciatingly slow search\n",
    "\n",
    "In class we have presented a _slow_ version of search that, given a search query $q$, loops over every document in the database\n",
    "computing the cosine similarity between document and query. Once this is done, it sorts documents by their similarity w.r.t. $q$ and returns the top $r$\n",
    "scoring ones. \n",
    "\n",
    "```\n",
    "1. for each d in D:\n",
    "    sim(d,q) = 0\n",
    "    get vector representing d\n",
    "    for each w in q:\n",
    "        sim(d,q) += tf(d,w) * idf(w)\n",
    "    normalize sim(d,q) by |d|*|q|\n",
    "2. sort results by similarity\n",
    "3. return top r docs\n",
    "```\n",
    "\n",
    "A possible implementation can be found below. \n",
    "\n",
    "__Remark:__ _It should be important to note that there are certain elements in the implementation below that refer to my own\n",
    "implementation, and that you should adapt to your own; in particular, the line_\n",
    "\n",
    "```    weights = dict(normalize(tf_idf(s['_id'])))   # gets weights as a python dict of term -> weight ```\n",
    "\n",
    "_obtains tf-idf weights through calling a function `tf_idf` that I have implemented that, given a docid, returns a list of pairs (term, weight); and `normalize` takes such a list a normalizes weights so that the corresponding vector has length 1. \n",
    "Obviously, you should adapt the code to your own implementations from previous sessions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "import tqdm\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(idx: str, client: Elasticsearch, doc_id: str, D: int) -> list:\n",
    "    \"\"\"\n",
    "    Compute tf-idf for each term in a document with internal id doc_id\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    tv = client.termvectors(index=idx, id=doc_id, fields=['text'], term_statistics=True)\n",
    "    tfidf = []\n",
    "    \n",
    "    if 'text' in tv['term_vectors']:\n",
    "        max_word = max(tv['term_vectors']['text']['terms'], key=lambda x: tv['term_vectors']['text']['terms'][x]['term_freq'])\n",
    "        max_fdj = tv['term_vectors']['text']['terms'][max_word]['term_freq']\n",
    "        \n",
    "        for word in tv['term_vectors']['text']['terms']:\n",
    "        \n",
    "            fdi = tv['term_vectors']['text']['terms'][word]['term_freq']    # term frequency in document\n",
    "            dfi = tv['term_vectors']['text']['terms'][word]['doc_freq']     # number of documents containing term in entire corpus\n",
    "            \n",
    "            tf = fdi/max_fdj\n",
    "            idf = math.log(D/dfi, 2)\n",
    "            \n",
    "            tfidf.append((word, tf*idf))\n",
    "            \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tdfidf: list) -> list:\n",
    "    \"\"\"\n",
    "    Normalize tf-idf weights so that the resulting vector has length 1\n",
    "    \"\"\"\n",
    "\n",
    "    import math\n",
    "\n",
    "    norm_d = math.sqrt(sum([w**2 for _, w in tdfidf]))\n",
    "    return [(t, w/norm_d) for t, w in tdfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "# r = 10  # only return r top docs\n",
    "# query = 'computer magic'\n",
    "# sims = dict()\n",
    "\n",
    "# l2query  = np.sqrt(len(query.split()))  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "# # get nr. of docs; just for the progress bar\n",
    "# ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])  # D\n",
    "\n",
    "# # scan through docs, compute cosine sim between query and each doc\n",
    "# for s in tqdm.tqdm(scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "#     docid = s['_source']['path']   # use path as id\n",
    "\n",
    "#     sims[docid] = 0.0\n",
    "#     weights = dict(normalize(tf_idf('arxiv', client, s['_id'], ndocs)))  # normalize weights for doc\n",
    "\n",
    "#     for w in query.split():  # gets terms as a list\n",
    "#         if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "#             sims[docid] += weights[w]   # accumulates if w in current doc\n",
    "\n",
    "#     # normalize sim\n",
    "#     sims[docid] /= l2query  # ||q||_2 = 1\n",
    "\n",
    "# # now sort by cosine similarity\n",
    "# sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "# pprint(sorted_answer[:r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch.helpers import scan\n",
    "# from pprint import pprint\n",
    "# from elasticsearch import Elasticsearch\n",
    "# import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# # get tf-idf vector from doc (internal) id\n",
    "# def tf_idf(doc_id):\n",
    "#     # does nothing, adapt to your needs\n",
    "#     return []\n",
    "\n",
    "# # normalizes weights so that resulting vec has length 1\n",
    "# def normalize(l1):\n",
    "#     # does nothing, adapt to your needs\n",
    "#     return l1\n",
    "\n",
    "# client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "# r = 10  # only return r top docs\n",
    "# query = 'computer magic'\n",
    "# sims = dict()\n",
    "\n",
    "# l2query  = np.sqrt(len(query.split()))  # l2 of query assuming 0-1 vector representation\n",
    "\n",
    "# # get nr. of docs; just for the progress bar\n",
    "# ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])\n",
    "\n",
    "# # scan through docs, compute cosine sim between query and each doc\n",
    "# for s in tqdm.tqdm(scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "#     docid = s['_source']['path']   # use path as id\n",
    "    \n",
    "#     sims[docid] = 0.0\n",
    "#     weights = dict(normalize(tf_idf(s['_id'])))   # get tf-idf weights representing doc as dict\n",
    "#     for w in query.split():  # gets terms as a list\n",
    "#         if w in weights:    # probably need to do something fancier to make sure that word is in vocabulary etc.\n",
    "#             sims[docid] += weights[w]   # accumulates if w in current doc\n",
    "#     # normalize sim\n",
    "#     sims[docid] /= l2query\n",
    "\n",
    "# # now sort by cosine similarity\n",
    "# sorted_answer = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "# pprint(sorted_answer[:r])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nz = len([x for x, s in sorted_answer if s>0])\n",
    "# total = len(sorted_answer)\n",
    "# print(f'There are {nz} docs with non-zero similarity out of {total}, i.e. {100.0*nz/total:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your tasks\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1:**  \n",
    "\n",
    "Make sure you understand the algorithm for implementing search described in the lecture notes. Both slow and efficient versions. Describe\n",
    "the number of sums you need to do in both slow and quick versions for the following toy example with a vocabulary of size 4 and four documents:\n",
    "\n",
    "- $q = 0,1,1,0$\n",
    "\n",
    "- document-term matrix:\n",
    "<center>\n",
    "\n",
    "\n",
    "|        | t1  | t2  | t3  | t4  |\n",
    "|--------|-----|-----|-----|-----|\n",
    "| **d1** | 1.2 | 0.0 | 0.0 | 0.0 |\n",
    "| **d2** | 0.7 | 0.3 | 1.5 | 0.1 |\n",
    "| **d3** | 0.0 | 0.0 | 0.0 | 0.7 |\n",
    "| **d4** | 2.0 | 0.0 | 0.0 | 0.0 |\n",
    "\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "If we use the slow (inefficient) algorithm to implement the vectorial model, we will need to perform a total of 8 additions. In this version, we traverse the corpus document by document, and for each document, we need to calculate the similarity with the corresponding word from the query q. In this case, with 4 documents and the query containing 2 words (t2, t3), for each document, we have to perform 2 additions,  which totals 4x2 = 8 additions.\n",
    "\n",
    "On the other hand, in the fast algorithm, where we use the inverted file, the total number of additions we need to make is 2. This is because we now traverse the weight matrix by columns/terms instead of rows/documents. For each word in the query, we extract its posting list (a list of documents that contain it), and for these documents, we calculate the partial similarity of the query word with the word in each respective document. Thus, instead of having the entire similarity for some documents at a single point $i$ in the execution, we have a partial similarity for all documents. In this case, we would first take the word t2, and since it only appears in document d2, we would perform 1 addition. Next, we would do the same for t3, which again only appears in d2, resulting in one more addition, making a total of 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 2:**\n",
    "\n",
    "Implement the quick version; run both slow and quick versions and report times (as a reference, in my old laptop it takes around 5m30s to run the slow version in the code above). Make sure both versions return the same answer. Note that you will need to build an inverted index in order to implement the efficient version as explained in class; it may take time but this is done once for all queries, and can be done \"off-line\".\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58102/58102 [02:35<00:00, 374.65it/s]\n"
     ]
    }
   ],
   "source": [
    "ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])\n",
    "posting_list = dict()\n",
    "\n",
    "for s in tqdm.tqdm(scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}}), total=ndocs):\n",
    "  \n",
    "  docid = s['_id']\n",
    "  tv = client.termvectors(index='arxiv', id=docid, fields=['text'], term_statistics=True)\n",
    "  \n",
    "  if 'text' in tv['term_vectors']:\n",
    "    for t in tv['term_vectors']['text']['terms']:\n",
    "      if t not in posting_list:\n",
    "        posting_list[t] = set()\n",
    "      posting_list[t].add(docid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('5t2UMosBHh_bflI4hWSJ', 0.47752597051076895),\n",
       " ('Ad2UMosBHh_bflI4hWWJ', 0.477263052236843),\n",
       " ('h9yUMosBHh_bflI4M89P', 0.38792349570755896),\n",
       " ('H92UMosBHh_bflI4h2gT', 0.3023262889426185),\n",
       " ('G92UMosBHh_bflI4bi3m', 0.26483163493536094),\n",
       " ('aN2UMosBHh_bflI4ayQI', 0.26483163493536094),\n",
       " ('Xt2UMosBHh_bflI4ayQI', 0.24832299488605178),\n",
       " ('4N2UMosBHh_bflI4bizm', 0.24832299488605178),\n",
       " ('j9yTMosBHh_bflI48I7S', 0.24062631526975578),\n",
       " ('29yTMosBHh_bflI47Ioz', 0.23778079125705068)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverted_file_implementation(query: str,  \n",
    "                                 client: Elasticsearch,\n",
    "                                 posting_list: dict, \n",
    "                                 ndocs:int, \n",
    "                                 r: int) -> list:\n",
    "  \"\"\"\n",
    "  Implement inverted file retrieval for a query and return top r results\n",
    "  \"\"\"\n",
    "  sims = dict()\n",
    "  \n",
    "  for w in tqdm.tqdm(query.split()):\n",
    "    L = posting_list[w]\n",
    "    for d in L:\n",
    "      weights = dict(normalize(tf_idf('arxiv', client, d, ndocs)))\n",
    "      if d not in sims:\n",
    "        sims[d] = 0.0 \n",
    "      sims[d] += weights[w]\n",
    "  \n",
    "  l2query = np.sqrt(len(query.split()))  \n",
    "  for d in sims:\n",
    "    sims[d] /= l2query\n",
    "    \n",
    "  sorted_by_similarity = sorted(sims.items(), key=lambda kv: kv[1], reverse=True)\n",
    "  return sorted_by_similarity[:r]\n",
    "\n",
    "query = 'computer magic'\n",
    "r = 10\n",
    "idx = 'arxiv'\n",
    "ndocs = int(client.cat.count(index='arxiv', format = \"json\")[0]['count'])\n",
    "\n",
    "inverted_file_implementation(query, client, posting_list, ndocs, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Compare the results for a few sample queries that you get from your quick version and ElasticSearch search. Do you get similar results? Which is faster?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Carlos\n",
      "[nltk_data]     Arbonés\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted file implementation took 0.43 seconds\n",
      "Elasticsearch implementation took 0.03 seconds \n",
      "\n",
      "Lets compare the results of the two implementations: \n",
      "\n",
      "Inverted file implementation: 5t2UMosBHh_bflI4hWSJ with score 0.47752597051076895\n",
      "Elasticsearch implementation: 5t2UMosBHh_bflI4hWSJ with score 14.512409 \n",
      "\n",
      "Inverted file implementation: Ad2UMosBHh_bflI4hWWJ with score 0.477263052236843\n",
      "Elasticsearch implementation: Ad2UMosBHh_bflI4hWWJ with score 14.512409 \n",
      "\n",
      "Inverted file implementation: h9yUMosBHh_bflI4M89P with score 0.38792349570755896\n",
      "Elasticsearch implementation: h9yUMosBHh_bflI4M89P with score 12.0883665 \n",
      "\n",
      "Inverted file implementation: H92UMosBHh_bflI4h2gT with score 0.3023262889426185\n",
      "Elasticsearch implementation: aN2UMosBHh_bflI4ayQI with score 11.981175 \n",
      "\n",
      "Inverted file implementation: G92UMosBHh_bflI4bi3m with score 0.26483163493536094\n",
      "Elasticsearch implementation: G92UMosBHh_bflI4bi3m with score 11.981175 \n",
      "\n",
      "Inverted file implementation: aN2UMosBHh_bflI4ayQI with score 0.26483163493536094\n",
      "Elasticsearch implementation: Xt2UMosBHh_bflI4ayQI with score 11.921848 \n",
      "\n",
      "Inverted file implementation: Xt2UMosBHh_bflI4ayQI with score 0.24832299488605178\n",
      "Elasticsearch implementation: 4N2UMosBHh_bflI4bizm with score 11.921848 \n",
      "\n",
      "Inverted file implementation: 4N2UMosBHh_bflI4bizm with score 0.24832299488605178\n",
      "Elasticsearch implementation: ad2UMosBHh_bflI4cjQD with score 11.408539 \n",
      "\n",
      "Inverted file implementation: j9yTMosBHh_bflI48I7S with score 0.24062631526975578\n",
      "Elasticsearch implementation: H92UMosBHh_bflI4h2gT with score 11.179635 \n",
      "\n",
      "Inverted file implementation: 29yTMosBHh_bflI47Ioz with score 0.23778079125705068\n",
      "Elasticsearch implementation: 0N2UMosBHh_bflI4h2gT with score 11.029575 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import random\n",
    "\n",
    "def compare_query_execution_times(query: str, \n",
    "                                  client: Elasticsearch, \n",
    "                                  posting_list: dict, \n",
    "                                  ndocs:int,\n",
    "                                  r: int) -> list:\n",
    "  \"\"\"\n",
    "  Compare query execution times for the two implementations\n",
    "  \"\"\"\n",
    "  import time\n",
    "  \n",
    "  start = time.time()\n",
    "  Own_implementation = inverted_file_implementation(query, client, posting_list, ndocs, r)\n",
    "  end = time.time()\n",
    "  print(f'Inverted file implementation took {end-start:.2f} seconds')\n",
    "  \n",
    "  start = time.time()\n",
    "  s = Search(using=client, index='arxiv')\n",
    "  q = Q('query_string', query=query)\n",
    "  s = s.query(q)\n",
    "  Elasticsearch_implementation = s[:r].execute()\n",
    "  end = time.time()\n",
    "  print(f'Elasticsearch implementation took {end-start:.2f} seconds \\n')\n",
    "\n",
    "  print(f'Let''s compare the results of the two implementations: \\n')\n",
    "  for i in range(r): \n",
    "    print(f'Inverted file implementation: {Own_implementation[i][0]} with score {Own_implementation[i][1]}')\n",
    "    print(f'Elasticsearch implementation: {Elasticsearch_implementation[i].meta.id} with score {Elasticsearch_implementation[i].meta.score} \\n')\n",
    "\n",
    "\n",
    "query = 'computer magic'\n",
    "compare_query_execution_times(query, client, posting_list, ndocs, r)\n",
    "\n",
    "# english_words = words.words()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rules of delivery\n",
    "\n",
    "- To be solved in _pairs_.\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session.\n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report listing the solutions to the exercises proposed. Include things like the important parts of your implementation (data structures used for representing objects, algorithms used, etc). You are welcome to add conclusions and findings that depart from what we asked you to do. We encourage you to discuss the difficulties you find; this lets us give you help and also improve the lab session for future editions.\n",
    "\n",
    "- Turn the report to PDF. Make sure it has your names, date, and title. Include your code in your submission.\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html) _before November 6th, 2023_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
