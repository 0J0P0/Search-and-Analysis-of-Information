{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 2: Intro to ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will learn:\n",
    "\n",
    "- a few basics of the `ElasticSearch` database\n",
    "- how to index a set of documents and how to ask simple queries about these documents\n",
    "- how to do this from `Python`\n",
    "- based on the previous, you will compute the boolean and tf-idf matrix for the toy corpus used in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ElasticSearch\n",
    "\n",
    "[ElasticSearch](https://www.elastic.co/) is a _NoSQL/document_ database with the capability of indexing and searching text documents. As a rough analogue, we can use the following table for the equivalence between ElasticSearch and a more classical relational database:\n",
    "\n",
    "| Relational DB | ElasticSearch |\n",
    "|---|---|\n",
    "| Database | Index |\n",
    "| Table | Type |\n",
    "| Row / record | Document |\n",
    "| Column | Field |\n",
    "\n",
    "An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data.\n",
    "\n",
    "`ElasticSearch` is a pretty big beast with many options. Luckily, there is much documentation, a few useful links are:\n",
    "\n",
    "- Here is the [full documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)\n",
    "- Intros you may want to have a look at: \n",
    "    - https://medium.com/expedia-group-tech/getting-started-with-elastic-search-6af62d7df8dd\n",
    "    - http://joelabrahamsson.com/elasticsearch-101\n",
    "- You found another one that you liked? Let us know. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running ElasticSearch\n",
    "\n",
    "First you will need to install `ElasticSearch` following instructions in their [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html).\n",
    "\n",
    "This database runs as a web service in a machine and can be accessed using a REST web API; however we will interact with the database through its python libraries `elasticsearch-py` and `elasticsearch-dsl`, so you will need to install these as well.  You can run `ElasticSearch` by typing from the command-line prompt:\n",
    "\n",
    "```\n",
    "$ <path_to_elasticsearch_bin>/elasticsearch &\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds (and a lot of logging) the database will be up and running; you may need to hit return for the prompt to show up. To test whether `ElasticSearch` is working execute the code in the cell below. __The database needs to be running throughout the execution of this script, otherwise you will get a connection error.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'{\\n  \"name\" : \"JPC2\",\\n  \"cluster_name\" : \"elasticsearch\",\\n  \"cluster_uuid'\n",
      " b'\" : \"H7sZwRSJTPC-bDvCrKVnLA\",\\n  \"version\" : {\\n    \"number\" : \"8.10.2\",\\n '\n",
      " b'   \"build_flavor\" : \"default\",\\n    \"build_type\" : \"zip\",\\n    \"build_hash'\n",
      " b'\" : \"6d20dd8ce62365be9b1aca96427de4622e970e9e\",\\n    \"build_date\" : \"2023'\n",
      " b'-09-19T08:16:24.564900370Z\",\\n    \"build_snapshot\" : false,\\n    \"lucene_v'\n",
      " b'ersion\" : \"9.7.0\",\\n    \"minimum_wire_compatibility_version\" : \"7.17.0\",\\n'\n",
      " b'    \"minimum_index_compatibility_version\" : \"7.0.0\"\\n  },\\n  \"tagline\" : \"'\n",
      " b'You Know, for Search\"\\n}\\n')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp = requests.get('http://localhost:9200/')\n",
    "    pprint(resp.content)\n",
    "    \n",
    "except Exception:\n",
    "    print('elasticsearch is not running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `ElasticSearch` is working you will see an answer from the server; otherwise you will see a message indicating that it is not running. You can try also throwing the URL http://localhost:9200 to your browser; you should get a similar answer.\n",
    "\n",
    "**In version 8 they introduced enhanced security, which may give you trouble when executing the code here, to deal with this you can either install an earlier version (7 or older) or turn off security settings in their `config/elasticsearch.yml` config file (just set to _false_ everything concerning the security options).** Since we are using the database in offline, local mode this should not be a problem.\n",
    "\n",
    "Also, you should run this script locally in your machine, if you use Google Collab or similar this is not going to work because elasticsearch should be running on the machine where the script is being executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing and querying\n",
    "\n",
    "`ElasticSearch` is a database that allows storing documents (tables do not need a predefined schema as in relational databases). Text in these documents can be processed so the queries extend beyond exact matches allowing complex queries, fuzzy matching and ranking documents respect to the actual match. \n",
    "\n",
    "These kinds of databases are behind search engines like Google Search or Bing.\n",
    "\n",
    "There are different ways of operating with ElasticSearch. It is deployed esentially as a web service with a REST API, so it can be accessed basically from any language with a library for operating with HTTP servers.\n",
    "\n",
    "We are going to use two python libraries for programming on top of ElasticSearch: `elasticsearch` and `elasticsearch-dsl`. Both provide access to ElasticSearch functionalities hiding and making more programming-friendly the interactions, the second one is more convenient for configurating and searching. Make sure both python libraries are installed to proceed with this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to see the essential elements for developing the session but feel free to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with ElasticSearch with need a client object of type `Elasticsearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpniz\\AppData\\Local\\Temp\\ipykernel_9428\\2120730465.py:3: DeprecationWarning: The 'timeout' parameter is deprecated in favor of 'request_timeout'\n",
      "  client = Elasticsearch(\"http://localhost:9200\", timeout=1000)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this client you have a connection for operating with Elastic search. Now we will create an index. There are index operations in each library, but the one in `elasticseach-dsl` is simpler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Index\n",
    "\n",
    "index = Index('test', using=client)  # if it does not exist, it is created; if it does exist, then it connects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need some text to index, for testing purposes we are going to use the python library `loremipsum`. We will need to install it first if it is not installed already, uncomment the code in next cell if you need to install the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create some random paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "('Quaerat sit dolor non consectetur magnam quisquam porro. Dolorem est '\n",
      " 'adipisci dolore sed. Tempora quiquia quiquia velit quaerat voluptatem est '\n",
      " 'voluptatem. Dolor voluptatem sit eius velit etincidunt ut. Adipisci ipsum '\n",
      " 'modi porro eius ipsum neque non. Tempora est eius quiquia aliquam.')\n"
     ]
    }
   ],
   "source": [
    "import lorem\n",
    "\n",
    "texts = [lorem.paragraph() for _ in range(10)]\n",
    "print(len(texts))\n",
    "pprint(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can index the paragraphs in ElasticSearch using the `index` method. The document is passed as a python dictionary with the `document` parameter. The keys of the dictionary will be the fields of the document, in this case we well have only one (`text`) -- here, we use this tag but could use anything we wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing new text: Quaerat sit dolor non consectetur magnam quisquam porro. Dolorem est a ...\n",
      "Indexing new text: Non modi ipsum quaerat dolorem adipisci. Modi numquam quaerat magnam d ...\n",
      "Indexing new text: Dolor neque amet aliquam ut dolore. Adipisci eius quaerat modi etincid ...\n",
      "Indexing new text: Numquam adipisci voluptatem quiquia amet aliquam. Voluptatem eius sed  ...\n",
      "Indexing new text: Voluptatem neque porro labore tempora neque labore consectetur. Porro  ...\n",
      "Indexing new text: Adipisci neque velit dolore sit dolore porro. Numquam quiquia sit temp ...\n",
      "Indexing new text: Velit labore magnam labore amet non labore dolor. Consectetur quisquam ...\n",
      "Indexing new text: Sit dolorem numquam aliquam neque labore tempora. Numquam aliquam cons ...\n",
      "Indexing new text: Eius modi aliquam dolor numquam dolor neque. Quiquia magnam dolore tem ...\n",
      "Indexing new text: Ut amet dolor est neque dolorem modi. Quaerat dolore non tempora porro ...\n"
     ]
    }
   ],
   "source": [
    "for t in texts:\n",
    "    client.index(index='test', document={'text': t})\n",
    "    print(f'Indexing new text: {t[:70]} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to get all docs in the index, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 30 hits:\n",
      "{'text': 'Ipsum labore amet dolore aliquam. Porro ut sit dolorem dolor ut. '\n",
      "         'Numquam ipsum tempora dolore etincidunt tempora neque tempora. '\n",
      "         'Quisquam sed numquam consectetur sit labore. Dolore numquam modi '\n",
      "         'quaerat eius quiquia. Sed sed velit modi. Neque sit amet quisquam '\n",
      "         'velit ut ipsum ut. Modi tempora modi sit. Dolore tempora quisquam ut '\n",
      "         'aliquam modi porro.'}\n",
      "{'text': 'Tempora ut consectetur porro neque tempora. Neque eius adipisci '\n",
      "         'quisquam labore. Dolor est sit quaerat eius velit porro. Est dolorem '\n",
      "         'dolorem quisquam. Dolorem dolore amet magnam porro dolorem dolor '\n",
      "         'est. Labore aliquam modi numquam dolore est consectetur magnam. Sit '\n",
      "         'dolore ipsum magnam.'}\n",
      "{'text': 'Aliquam amet dolore quisquam consectetur numquam dolorem neque. '\n",
      "         'Consectetur quiquia quaerat amet amet tempora ut. Amet consectetur '\n",
      "         'quiquia dolorem neque consectetur numquam. Non eius eius magnam '\n",
      "         'dolor quisquam sed. Voluptatem velit est sit ipsum. Neque non '\n",
      "         'quisquam est. Voluptatem eius modi voluptatem ipsum dolorem quiquia. '\n",
      "         'Labore non neque adipisci dolorem numquam velit.'}\n",
      "{'text': 'Dolorem aliquam dolorem dolore consectetur ut dolore. Labore porro '\n",
      "         'magnam etincidunt. Quiquia etincidunt ipsum voluptatem sit ut '\n",
      "         'voluptatem sed. Numquam modi velit magnam dolorem. Ipsum sed labore '\n",
      "         'ut tempora sit porro sed. Dolor etincidunt quaerat quisquam modi '\n",
      "         'dolorem amet.'}\n",
      "{'text': 'Ipsum velit dolor amet sed consectetur. Dolorem aliquam amet non '\n",
      "         'ipsum adipisci labore. Sit sit ut eius numquam non amet ut. Ipsum '\n",
      "         'neque voluptatem neque quaerat. Adipisci ipsum dolor sit modi velit. '\n",
      "         'Quaerat ut est magnam dolor eius non magnam. Adipisci porro eius '\n",
      "         'quaerat.'}\n",
      "{'text': 'Sed quiquia etincidunt neque. Quiquia consectetur adipisci neque. '\n",
      "         'Labore eius ipsum voluptatem. Adipisci aliquam sit ipsum dolorem. '\n",
      "         'Eius quaerat quaerat quiquia etincidunt etincidunt non tempora.'}\n",
      "{'text': 'Ut quaerat adipisci magnam. Est dolor aliquam dolorem numquam eius '\n",
      "         'quisquam etincidunt. Dolore magnam eius ut voluptatem est. Neque '\n",
      "         'consectetur amet quisquam etincidunt sed numquam neque. Porro porro '\n",
      "         'numquam ut quisquam. Non porro aliquam eius consectetur non. Neque '\n",
      "         'velit neque tempora porro. Quiquia dolorem quaerat aliquam velit '\n",
      "         'magnam adipisci.'}\n",
      "{'text': 'Quaerat quiquia magnam neque modi neque. Numquam quiquia voluptatem '\n",
      "         'est. Ut ipsum eius dolor non numquam aliquam. Non dolor sit adipisci '\n",
      "         'non amet velit sit. Neque dolorem dolor labore non adipisci dolorem '\n",
      "         'numquam. Quaerat aliquam magnam quaerat. Numquam non quisquam '\n",
      "         'quisquam. Tempora porro sed ipsum eius. Est est quiquia quiquia. '\n",
      "         'Numquam magnam aliquam etincidunt est aliquam eius consectetur.'}\n",
      "{'text': 'Neque tempora velit labore etincidunt quisquam tempora amet. Dolor '\n",
      "         'quaerat porro dolorem quaerat dolor. Modi quiquia quaerat adipisci. '\n",
      "         'Neque sed dolor tempora. Quiquia eius ut sit etincidunt aliquam. '\n",
      "         'Numquam non dolor aliquam labore dolor quiquia. Amet modi etincidunt '\n",
      "         'modi velit ipsum. Velit dolorem neque dolor labore. Sed quaerat amet '\n",
      "         'est non magnam. Non dolorem dolorem adipisci ut dolorem quiquia.'}\n",
      "{'text': 'Neque dolorem dolor est voluptatem. Voluptatem ut numquam modi '\n",
      "         'tempora tempora modi tempora. Tempora voluptatem ut tempora sit '\n",
      "         'dolore neque neque. Velit sed porro aliquam tempora est quiquia sit. '\n",
      "         'Voluptatem non quaerat velit dolore modi dolore est. Quiquia quiquia '\n",
      "         'sit amet est porro est labore. Est sit non eius quisquam tempora. '\n",
      "         'Amet est sit consectetur non voluptatem. Porro velit neque numquam. '\n",
      "         'Tempora quisquam dolorem tempora numquam non.'}\n"
     ]
    }
   ],
   "source": [
    "# get all docs in index 'test'\n",
    "resp = client.search(index=\"test\", query={\"match_all\": {}})\n",
    "\n",
    "# print them\n",
    "print(f\"Got {resp['hits']['total']['value']} hits:\")\n",
    "for hit in resp['hits']['hits']:\n",
    "    pprint(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for documents that contain a given keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 matches.\n",
      "\n",
      "ID: 79oy8YoBieWG3-SLODtk\n",
      "Text: Quaerat quiquia magnam neque modi neque. Numquam quiquia voluptatem est. Ut ipsum eius dolor non numquam aliquam. Non dolor sit adipisci non amet velit sit. Neque dolorem dolor labore non adipisci dolorem numquam. Quaerat aliquam magnam quaerat. Numquam non quisquam quisquam. Tempora porro sed ipsum eius. Est est quiquia quiquia. Numquam magnam aliquam etincidunt est aliquam eius consectetur.\n",
      "\n",
      "ID: _toy8YoBieWG3-SLTDse\n",
      "Text: Non magnam dolorem est velit. Adipisci ipsum non quisquam eius adipisci. Voluptatem numquam non labore tempora. Amet quisquam porro porro tempora amet. Modi velit velit numquam porro labore sed. Ipsum numquam dolore ut dolor velit. Consectetur amet quiquia neque dolorem non eius magnam.\n",
      "\n",
      "ID: ANoy8YoBieWG3-SLTDws\n",
      "Text: Magnam quaerat sed porro modi eius. Tempora etincidunt sit sit. Non ipsum neque consectetur magnam non numquam. Voluptatem dolore magnam porro. Consectetur quisquam velit quaerat porro. Dolor quaerat magnam neque quiquia modi ipsum non. Sit dolorem tempora ipsum. Dolor aliquam sed tempora aliquam dolorem est amet. Modi non labore sit sed consectetur labore.\n",
      "\n",
      "ID: Atoy8YoBieWG3-SLTDw6\n",
      "Text: Ut consectetur quiquia ipsum est. Ut modi quisquam etincidunt. Est quaerat non labore adipisci non consectetur. Dolorem etincidunt etincidunt dolore amet voluptatem adipisci labore. Magnam quisquam magnam ipsum modi. Velit sit porro magnam non quaerat neque etincidunt. Ipsum dolore consectetur consectetur sed consectetur dolorem. Magnam tempora sit dolore voluptatem aliquam dolore. Amet non eius consectetur.\n",
      "\n",
      "ID: 7Noy8YoBieWG3-SLODtR\n",
      "Text: Ipsum velit dolor amet sed consectetur. Dolorem aliquam amet non ipsum adipisci labore. Sit sit ut eius numquam non amet ut. Ipsum neque voluptatem neque quaerat. Adipisci ipsum dolor sit modi velit. Quaerat ut est magnam dolor eius non magnam. Adipisci porro eius quaerat.\n",
      "\n",
      "ID: 8doy8YoBieWG3-SLODtw\n",
      "Text: Neque dolorem dolor est voluptatem. Voluptatem ut numquam modi tempora tempora modi tempora. Tempora voluptatem ut tempora sit dolore neque neque. Velit sed porro aliquam tempora est quiquia sit. Voluptatem non quaerat velit dolore modi dolore est. Quiquia quiquia sit amet est porro est labore. Est sit non eius quisquam tempora. Amet est sit consectetur non voluptatem. Porro velit neque numquam. Tempora quisquam dolorem tempora numquam non.\n",
      "\n",
      "ID: G9oy8YoBieWG3-SLpDxu\n",
      "Text: Modi neque non voluptatem non numquam. Modi etincidunt porro numquam ipsum tempora. Aliquam amet quaerat eius est. Quisquam modi magnam etincidunt tempora adipisci. Labore quisquam numquam magnam modi magnam.\n",
      "\n",
      "ID: 6toy8YoBieWG3-SLODtH\n",
      "Text: Aliquam amet dolore quisquam consectetur numquam dolorem neque. Consectetur quiquia quaerat amet amet tempora ut. Amet consectetur quiquia dolorem neque consectetur numquam. Non eius eius magnam dolor quisquam sed. Voluptatem velit est sit ipsum. Neque non quisquam est. Voluptatem eius modi voluptatem ipsum dolorem quiquia. Labore non neque adipisci dolorem numquam velit.\n",
      "\n",
      "ID: HNoy8YoBieWG3-SLpDx0\n",
      "Text: Adipisci dolorem dolor magnam adipisci. Consectetur labore consectetur quiquia. Sed consectetur quaerat ipsum consectetur numquam quaerat non. Quiquia non modi quiquia numquam magnam tempora voluptatem. Modi labore dolor magnam tempora.\n",
      "\n",
      "ID: 8Noy8YoBieWG3-SLODtr\n",
      "Text: Neque tempora velit labore etincidunt quisquam tempora amet. Dolor quaerat porro dolorem quaerat dolor. Modi quiquia quaerat adipisci. Neque sed dolor tempora. Quiquia eius ut sit etincidunt aliquam. Numquam non dolor aliquam labore dolor quiquia. Amet modi etincidunt modi velit ipsum. Velit dolorem neque dolor labore. Sed quaerat amet est non magnam. Non dolorem dolorem adipisci ut dolorem quiquia.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch_dsl import Search\n",
    "\n",
    "# the following search query specifies the field where we want to search\n",
    "s_obj = Search(using=client, index='test')\n",
    "sq = s_obj.query('match', text='non')\n",
    "resp = sq.execute()\n",
    "\n",
    "print(f'Found {len(resp)} matches.')\n",
    "\n",
    "for hit in resp:\n",
    "    print(f'\\nID: {hit.meta.id}\\nText: {hit.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Counting words and docs\n",
    "\n",
    "`Elastic search` helps us to obtain the counts of words in each document. For example, the following code obtains the counts of words of a whole index by adding the counts of words obtained from each document through the functionality of `termvectors`. This function also allows us to get _document counts_ for computing tf-idf weights, by setting the `term_statistics` option to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from collections import Counter\n",
    "\n",
    "# Search for all the documents and query the list of (word, frequency) of each one\n",
    "# Totals are accumulated using a Counter for term frequencies, but not for document freqs.\n",
    "word_counts = Counter()\n",
    "doc_counts = Counter()\n",
    "sc = scan(client, index='test', query={\"query\" : {\"match_all\": {}}})\n",
    "for s in sc:\n",
    "    tv = client.termvectors(index='test', id=s['_id'], fields=['text'], term_statistics=True, positions=False)\n",
    "    if 'text' in tv['term_vectors']:   # just in case some document has no field named 'text'\n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            word = t\n",
    "            count = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            df = tv['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "            #pprint(tv['term_vectors']['text']['terms'][t])\n",
    "            word_counts.update({word: count})  # número de veces que aparece la palabra en el documento\n",
    "            doc_counts[word] = df      # número de documentos en los que aparece la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tempora', 67),\n",
       " ('modi', 61),\n",
       " ('ut', 60),\n",
       " ('consectetur', 58),\n",
       " ('sit', 58),\n",
       " ('ipsum', 57),\n",
       " ('neque', 57),\n",
       " ('magnam', 57),\n",
       " ('dolorem', 55),\n",
       " ('eius', 55),\n",
       " ('quisquam', 55),\n",
       " ('numquam', 54),\n",
       " ('est', 54),\n",
       " ('dolor', 52),\n",
       " ('quaerat', 51),\n",
       " ('velit', 51),\n",
       " ('dolore', 50),\n",
       " ('adipisci', 50),\n",
       " ('non', 50),\n",
       " ('labore', 49),\n",
       " ('quiquia', 48),\n",
       " ('porro', 44),\n",
       " ('etincidunt', 43),\n",
       " ('amet', 41),\n",
       " ('sed', 40),\n",
       " ('voluptatem', 39),\n",
       " ('aliquam', 38)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show word frequencies\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consectetur', 37),\n",
       " ('tempora', 37),\n",
       " ('quaerat', 36),\n",
       " ('eius', 35),\n",
       " ('neque', 35),\n",
       " ('porro', 35),\n",
       " ('aliquam', 34),\n",
       " ('dolorem', 34),\n",
       " ('modi', 34),\n",
       " ('ipsum', 33),\n",
       " ('labore', 33),\n",
       " ('numquam', 33),\n",
       " ('ut', 33),\n",
       " ('magnam', 33),\n",
       " ('dolor', 32),\n",
       " ('quisquam', 32),\n",
       " ('sed', 32),\n",
       " ('velit', 32),\n",
       " ('dolore', 31),\n",
       " ('adipisci', 31),\n",
       " ('est', 31),\n",
       " ('quiquia', 30),\n",
       " ('sit', 30),\n",
       " ('amet', 29),\n",
       " ('voluptatem', 29),\n",
       " ('non', 27),\n",
       " ('etincidunt', 26)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show doc freq\n",
    "doc_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Proposed simple exercise\n",
    "\n",
    "To get more familiar with elasticsearch, we propose that you _generate the Boolean and tf-idf matrices_ for the toy example that we used in class. You will find 7 text documents that contain the toy documents with the materials for this session in the racó. The steps to follow are:\n",
    "\n",
    "- create an empty index\n",
    "- open each text document in the `toy-docs` folder provided, read its contents and add it to the index as a new document; your index should contain 7 documents after this\n",
    "- use the `termvectors` function to obtain term and doc counts, generate Boolean and tf-idf matrices based on these counts\n",
    "- double check that your results coincide with the numbers in theory slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index('exercise', using=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 8):\n",
    "    with open('toy-docs/' + f'd{i}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        client.index(index='exercise', document={'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 hits\n"
     ]
    }
   ],
   "source": [
    "resp = client.search(index=\"exercise\", query={\"match_all\": {}})\n",
    "print(f\"Got {resp['hits']['total']['value']} hits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([], shape=(0, 6), dtype=int32)\n",
      "\n",
      "\n",
      "array([], shape=(0, 6), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "D = resp['hits']['total']['value']\n",
    "\n",
    "boolean_matrix = np.zeros((D, 6), dtype=int)\n",
    "w_matrix = np.zeros((D, 6))\n",
    "map = {'five': 0, 'four': 1, 'one': 2, 'six': 3, 'three': 4, 'two': 5}\t\n",
    "\n",
    "word_counts = Counter()\n",
    "doc_counts = Counter()\n",
    "sc = scan(client, index='exercise', query={\"query\" : {\"match_all\": {}}})\n",
    "\n",
    "for i, s in enumerate(sc):\n",
    "    tv = client.termvectors(index='exercise', \n",
    "                            id=s['_id'], \n",
    "                            fields=['text'], \n",
    "                            term_statistics=True, \n",
    "                            positions=False)\n",
    "\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        # get the word with the highest frequency and its frequency\n",
    "        max_word = max(tv['term_vectors']['text']['terms'], key=lambda x: tv['term_vectors']['text']['terms'][x]['term_freq'])\n",
    "        max_freq = tv['term_vectors']['text']['terms'][max_word]['term_freq']\n",
    "        \n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            word = t\n",
    "            \n",
    "            count = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            df = tv['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "            word_counts.update({word: count})  \n",
    "            doc_counts[word] = df\n",
    "\n",
    "            boolean_matrix[i][map[word]] = 1\n",
    "            w_matrix[i][map[word]] = (count/max_freq) * math.log(D/df, 2)\n",
    "\n",
    "\n",
    "pprint(boolean_matrix)\n",
    "print('\\n')\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "pprint(w_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Finally, we remove the test index.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
