{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 2: Intro to ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will learn:\n",
    "\n",
    "- a few basics of the `ElasticSearch` database\n",
    "- how to index a set of documents and how to ask simple queries about these documents\n",
    "- how to do this from `Python`\n",
    "- based on the previous, you will compute the boolean and tf-idf matrix for the toy corpus used in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ElasticSearch\n",
    "\n",
    "[ElasticSearch](https://www.elastic.co/) is a _NoSQL/document_ database with the capability of indexing and searching text documents. As a rough analogue, we can use the following table for the equivalence between ElasticSearch and a more classical relational database:\n",
    "\n",
    "| Relational DB | ElasticSearch |\n",
    "|---|---|\n",
    "| Database | Index |\n",
    "| Table | Type |\n",
    "| Row / record | Document |\n",
    "| Column | Field |\n",
    "\n",
    "An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data.\n",
    "\n",
    "`ElasticSearch` is a pretty big beast with many options. Luckily, there is much documentation, a few useful links are:\n",
    "\n",
    "- Here is the [full documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)\n",
    "- Intros you may want to have a look at: \n",
    "    - https://medium.com/expedia-group-tech/getting-started-with-elastic-search-6af62d7df8dd\n",
    "    - http://joelabrahamsson.com/elasticsearch-101\n",
    "- You found another one that you liked? Let us know. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running ElasticSearch\n",
    "\n",
    "First you will need to install `ElasticSearch` following instructions in their [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html).\n",
    "\n",
    "This database runs as a web service in a machine and can be accessed using a REST web API; however we will interact with the database through its python libraries `elasticsearch-py` and `elasticsearch-dsl`, so you will need to install these as well.  You can run `ElasticSearch` by typing from the command-line prompt:\n",
    "\n",
    "```\n",
    "$ <path_to_elasticsearch_bin>/elasticsearch &\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds (and a lot of logging) the database will be up and running; you may need to hit return for the prompt to show up. To test whether `ElasticSearch` is working execute the code in the cell below. __The database needs to be running throughout the execution of this script, otherwise you will get a connection error.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'{\\n  \"name\" : \"DESKTOP-QNOM0EQ\",\\n  \"cluster_name\" : \"elasticsearch\",\\n  \"c'\n",
      " b'luster_uuid\" : \"eig_fG5ySRWEscM_Y6LUcQ\",\\n  \"version\" : {\\n    \"number\" : '\n",
      " b'\"8.10.2\",\\n    \"build_flavor\" : \"default\",\\n    \"build_type\" : \"zip\",\\n    '\n",
      " b'\"build_hash\" : \"6d20dd8ce62365be9b1aca96427de4622e970e9e\",\\n    \"build_da'\n",
      " b'te\" : \"2023-09-19T08:16:24.564900370Z\",\\n    \"build_snapshot\" : false,\\n  '\n",
      " b'  \"lucene_version\" : \"9.7.0\",\\n    \"minimum_wire_compatibility_version\" :'\n",
      " b' \"7.17.0\",\\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\\n  },\\n  \"t'\n",
      " b'agline\" : \"You Know, for Search\"\\n}\\n')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp = requests.get('http://localhost:9200/')\n",
    "    pprint(resp.content)\n",
    "    \n",
    "except Exception:\n",
    "    print('elasticsearch is not running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `ElasticSearch` is working you will see an answer from the server; otherwise you will see a message indicating that it is not running. You can try also throwing the URL http://localhost:9200 to your browser; you should get a similar answer.\n",
    "\n",
    "**In version 8 they introduced enhanced security, which may give you trouble when executing the code here, to deal with this you can either install an earlier version (7 or older) or turn off security settings in their `config/elasticsearch.yml` config file (just set to _false_ everything concerning the security options).** Since we are using the database in offline, local mode this should not be a problem.\n",
    "\n",
    "Also, you should run this script locally in your machine, if you use Google Collab or similar this is not going to work because elasticsearch should be running on the machine where the script is being executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing and querying\n",
    "\n",
    "`ElasticSearch` is a database that allows storing documents (tables do not need a predefined schema as in relational databases). Text in these documents can be processed so the queries extend beyond exact matches allowing complex queries, fuzzy matching and ranking documents respect to the actual match. \n",
    "\n",
    "These kinds of databases are behind search engines like Google Search or Bing.\n",
    "\n",
    "There are different ways of operating with ElasticSearch. It is deployed esentially as a web service with a REST API, so it can be accessed basically from any language with a library for operating with HTTP servers.\n",
    "\n",
    "We are going to use two python libraries for programming on top of ElasticSearch: `elasticsearch` and `elasticsearch-dsl`. Both provide access to ElasticSearch functionalities hiding and making more programming-friendly the interactions, the second one is more convenient for configurating and searching. Make sure both python libraries are installed to proceed with this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to see the essential elements for developing the session but feel free to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with ElasticSearch with need a client object of type `Elasticsearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlos Arbon√©s\\AppData\\Local\\Temp\\ipykernel_9268\\2120730465.py:3: DeprecationWarning: The 'timeout' parameter is deprecated in favor of 'request_timeout'\n",
      "  client = Elasticsearch(\"http://localhost:9200\", timeout=1000)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this client you have a connection for operating with Elastic search. Now we will create an index. There are index operations in each library, but the one in `elasticseach-dsl` is simpler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Index\n",
    "\n",
    "index = Index('test', using=client)  # if it does not exist, it is created; if it does exist, then it connects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need some text to index, for testing purposes we are going to use the python library `loremipsum`. We will need to install it first if it is not installed already, uncomment the code in next cell if you need to install the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create some random paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "('Non tempora porro consectetur magnam. Modi sed ut aliquam tempora. Aliquam '\n",
      " 'eius etincidunt non porro. Neque tempora porro tempora quisquam ipsum '\n",
      " 'quiquia. Sit ipsum neque aliquam magnam etincidunt etincidunt est. Tempora '\n",
      " 'tempora tempora porro non non.')\n"
     ]
    }
   ],
   "source": [
    "import lorem\n",
    "\n",
    "texts = [lorem.paragraph() for _ in range(10)]\n",
    "print(len(texts))\n",
    "pprint(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can index the paragraphs in ElasticSearch using the `index` method. The document is passed as a python dictionary with the `document` parameter. The keys of the dictionary will be the fields of the document, in this case we well have only one (`text`) -- here, we use this tag but could use anything we wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing new text: Non tempora porro consectetur magnam. Modi sed ut aliquam tempora. Ali ...\n",
      "Indexing new text: Non sit modi dolore etincidunt quaerat ipsum neque. Voluptatem dolor i ...\n",
      "Indexing new text: Quisquam consectetur porro etincidunt. Etincidunt sit amet aliquam. Al ...\n",
      "Indexing new text: Dolore non quiquia modi amet velit velit. Non numquam non quiquia ut q ...\n",
      "Indexing new text: Ut magnam sed sed adipisci dolorem. Dolor ut ipsum porro magnam sed qu ...\n",
      "Indexing new text: Non quiquia etincidunt labore porro. Dolor quaerat magnam ipsum numqua ...\n",
      "Indexing new text: Amet dolorem quaerat dolorem. Numquam porro sit quiquia eius consectet ...\n",
      "Indexing new text: Sit modi eius consectetur sit voluptatem modi. Ipsum neque dolor modi  ...\n",
      "Indexing new text: Quaerat magnam amet quaerat. Amet amet quaerat dolorem amet. Tempora d ...\n",
      "Indexing new text: Ut porro numquam aliquam sit. Quiquia consectetur porro dolor quisquam ...\n"
     ]
    }
   ],
   "source": [
    "for t in texts:\n",
    "    client.index(index='test', document={'text': t})\n",
    "    print(f'Indexing new text: {t[:70]} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to get all docs in the index, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 hits:\n"
     ]
    }
   ],
   "source": [
    "# get all docs in index 'test'\n",
    "resp = client.search(index=\"test\", query={\"match_all\": {}})\n",
    "\n",
    "# print them\n",
    "print(f\"Got {resp['hits']['total']['value']} hits:\")\n",
    "for hit in resp['hits']['hits']:\n",
    "    pprint(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for documents that contain a given keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 matches.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch_dsl import Search\n",
    "\n",
    "# the following search query specifies the field where we want to search\n",
    "s_obj = Search(using=client, index='test')\n",
    "sq = s_obj.query('match', text='non')\n",
    "resp = sq.execute()\n",
    "\n",
    "print(f'Found {len(resp)} matches.')\n",
    "\n",
    "for hit in resp:\n",
    "    print(f'\\nID: {hit.meta.id}\\nText: {hit.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Counting words and docs\n",
    "\n",
    "`Elastic search` helps us to obtain the counts of words in each document. For example, the following code obtains the counts of words of a whole index by adding the counts of words obtained from each document through the functionality of `termvectors`. This function also allows us to get _document counts_ for computing tf-idf weights, by setting the `term_statistics` option to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from collections import Counter\n",
    "\n",
    "# Search for all the documents and query the list of (word, frequency) of each one\n",
    "# Totals are accumulated using a Counter for term frequencies, but not for document freqs.\n",
    "word_counts = Counter()\n",
    "doc_counts = Counter()\n",
    "sc = scan(client, index='test', query={\"query\" : {\"match_all\": {}}})\n",
    "for s in sc:\n",
    "    tv = client.termvectors(index='test', id=s['_id'], fields=['text'], term_statistics=True, positions=False)\n",
    "    if 'text' in tv['term_vectors']:   # just in case some document has no field named 'text'\n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            word = t\n",
    "            count = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            df = tv['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "            #pprint(tv['term_vectors']['text']['terms'][t])\n",
    "            word_counts.update({word: count})  # n√∫mero de veces que aparece la palabra en el documento\n",
    "            doc_counts[word] = df      # n√∫mero de documentos en los que aparece la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show word frequencies\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show doc freq\n",
    "doc_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Proposed simple exercise\n",
    "\n",
    "To get more familiar with elasticsearch, we propose that you _generate the Boolean and tf-idf matrices_ for the toy example that we used in class. You will find 7 text documents that contain the toy documents with the materials for this session in the rac√≥. The steps to follow are:\n",
    "\n",
    "- create an empty index\n",
    "- open each text document in the `toy-docs` folder provided, read its contents and add it to the index as a new document; your index should contain 7 documents after this\n",
    "- use the `termvectors` function to obtain term and doc counts, generate Boolean and tf-idf matrices based on these counts\n",
    "- double check that your results coincide with the numbers in theory slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index('exercise', using=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 8):\n",
    "    with open('toy-docs/' + f'd{i}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        client.index(index='exercise', document={'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 14 hits\n"
     ]
    }
   ],
   "source": [
    "resp = client.search(index='exercise', query={\"match_all\": {}})\n",
    "print(f\"Got {resp['hits']['total']['value']} hits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 1, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 1, 1],\n",
      "       [1, 1, 1, 0, 1, 0],\n",
      "       [0, 0, 1, 1, 1, 1],\n",
      "       [0, 1, 0, 1, 1, 0],\n",
      "       [0, 0, 0, 1, 1, 0],\n",
      "       [1, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 1, 1],\n",
      "       [1, 1, 1, 0, 1, 0],\n",
      "       [0, 0, 1, 1, 1, 1],\n",
      "       [0, 1, 0, 1, 1, 0],\n",
      "       [0, 0, 0, 1, 1, 0],\n",
      "       [1, 1, 0, 0, 0, 0]])\n",
      "\n",
      "\n",
      "array([[0.  , 0.  , 1.22, 0.  , 0.22, 0.  ],\n",
      "       [0.  , 0.  , 0.  , 0.  , 0.11, 1.81],\n",
      "       [1.81, 0.41, 0.41, 0.  , 0.07, 0.  ],\n",
      "       [0.  , 0.  , 0.31, 0.61, 0.06, 1.81],\n",
      "       [0.  , 1.22, 0.  , 0.41, 0.07, 0.  ],\n",
      "       [0.  , 0.  , 0.  , 0.81, 0.22, 0.  ],\n",
      "       [1.81, 1.22, 0.  , 0.  , 0.  , 0.  ],\n",
      "       [0.  , 0.  , 1.22, 0.  , 0.22, 0.  ],\n",
      "       [0.  , 0.  , 0.  , 0.  , 0.11, 1.81],\n",
      "       [1.81, 0.41, 0.41, 0.  , 0.07, 0.  ],\n",
      "       [0.  , 0.  , 0.31, 0.61, 0.06, 1.81],\n",
      "       [0.  , 1.22, 0.  , 0.41, 0.07, 0.  ],\n",
      "       [0.  , 0.  , 0.  , 0.81, 0.22, 0.  ],\n",
      "       [1.81, 1.22, 0.  , 0.  , 0.  , 0.  ]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "D = resp['hits']['total']['value']\n",
    "\n",
    "boolean_matrix = np.zeros((D, 6), dtype=int)\n",
    "w_matrix = np.zeros((D, 6))\n",
    "map = {'five': 0, 'four': 1, 'one': 2, 'six': 3, 'three': 4, 'two': 5}\t\n",
    "\n",
    "word_counts = Counter()\n",
    "doc_counts = Counter()\n",
    "sc = scan(client, index='exercise', query={\"query\" : {\"match_all\": {}}})\n",
    "\n",
    "for i, s in enumerate(sc):\n",
    "    tv = client.termvectors(index='exercise', \n",
    "                            id=s['_id'], \n",
    "                            fields=['text'], \n",
    "                            term_statistics=True, \n",
    "                            positions=False)\n",
    "\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        # get the word with the highest frequency and its frequency\n",
    "        max_word = max(tv['term_vectors']['text']['terms'], key=lambda x: tv['term_vectors']['text']['terms'][x]['term_freq'])\n",
    "        max_freq = tv['term_vectors']['text']['terms'][max_word]['term_freq']\n",
    "        \n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            word = t\n",
    "            \n",
    "            count = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            df = tv['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "            word_counts.update({word: count})  \n",
    "            doc_counts[word] = df\n",
    "\n",
    "            boolean_matrix[i][map[word]] = 1\n",
    "            w_matrix[i][map[word]] = (count/max_freq) * math.log(D/df, 2)\n",
    "\n",
    "\n",
    "pprint(boolean_matrix)\n",
    "print('\\n')\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "pprint(w_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Finally, we remove the test index.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
