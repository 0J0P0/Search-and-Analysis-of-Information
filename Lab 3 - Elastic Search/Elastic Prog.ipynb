{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 3: Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn how to tell ElasticSearch to apply different tokenizers and filters to the documents, like removing stopwords or stemming the words.\n",
    "- Study how these changes affect the terms that ElasticSearch puts in the index, and how this in turn affects searches.\n",
    "- Continuing previous work, implement tf-idf scheme over a repository of scietific article abstracts, including cosine measure for document similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing with ElasticSearch\n",
    "\n",
    "One of the tasks of the previous session was to remove from the documents vocabulary all those strings that were not proper words. Obviously this is a frequent task and all these kinds of DB have standard processes that help to filter and reduce the terms that are not useful for searching.\n",
    "\n",
    "Text, before being indexed, can be subjected to a pipeline of different processes that strips it from anything that will not be useful for a specific application. In ES these preprocessing pipelines are called _Analyzers_; ES includes many choices for each preprocessing step. \n",
    "\n",
    "\n",
    "The [following picture](https://www.elastic.co/es/blog/found-text-analysis-part-1) illustrates the chaining of preprocessing steps:\n",
    "\n",
    "![](https://api.contentstack.io/v2/assets/575e4c8c3dc542cb38c08267/download?uid=blt51e787daed39eae9?uid=blt51e787daed39eae9)\n",
    "\n",
    "The first step of the pipeline is usually a process that converts _raw text_ into _tokens_. We can for example tokenize a text using blanks and punctuation signs or use a language specific analyzer that detects words in an specific language or parse HTML/XML...\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) of the ElasticSearch manual explains the different text tokenizers available.\n",
    "\n",
    "Once we have obtained tokens, we can _normalize_ the strings and/or filter out valid tokens that are not useful. For instance, strings can be transformed to lowercase so all occurrences of the same word are mapped to the same token regardless of whether they were capitalized. Also, there are words that are not semantically useful when searching such as adverbs, articles or prepositions, in this case each language will have its own standard list of words; these are usually called \"_stopwords_\". Another language-specific token normalization is stemming. The stem of a word corresponds to the common part of a word from all variants are formed by inflection or addition of suffixes or prefixes. For instance, the words \"unstoppable\", \"stops\" and \"stopping\" all derive from the stem \"stop\". The idea is that all variations of a word will be represented by the same token.\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of ElasticSearch manual will give you an idea of the possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modifying `ElasticSearch` index behavior (using Analyzers)\n",
    "\n",
    "In this section we are going to learn how to set up preprocessing with ElasticSearch. We are going to do it _inline_ so that you have a few examples and get familiar with how to set up ES analyzers. We are going to showcase the different options with the toy English phrase\n",
    "\n",
    "```\n",
    "my taylor 4ís was% &printing printed rich the.\n",
    "```\n",
    "\n",
    "which contains symbols and weird things to see what effect the different tokenizers and filtering options have. We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `whitespace` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "\n",
    "# work with dummy index called 'foo'\n",
    "ind = Index('foo', using=client)\n",
    "ind.settings(number_of_shards=1)\n",
    "try:\n",
    "    # drop if exists\n",
    "    ind.delete()\n",
    "except NotFoundError:\n",
    "    pass\n",
    "\n",
    "# create it\n",
    "ind.create()\n",
    "\n",
    "# close to update analyzer to custom `my_analyzer`    \n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was%', 'start_offset': 14, 'end_offset': 18, 'type': 'word', 'position': 3}\n",
      "{'token': '&printing', 'start_offset': 19, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the.', 'start_offset': 42, 'end_offset': 46, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body=\n",
    "                  {'analyzer':'default',\n",
    "                   'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `standard` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': '<ALPHANUM>', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': '<ALPHANUM>', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': '<ALPHANUM>', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `letter` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'ís', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'is', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop` + `snowball`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'print', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'print', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** solve exercise 1 from problem set 1 using ElasticSearch. You can use the following string.\n",
    "\n",
    "```\n",
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = Index('foo', using=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase', 'stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'we', 'start_offset': 1, 'end_offset': 3, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'found', 'start_offset': 4, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': 'my', 'start_offset': 10, 'end_offset': 12, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'ladi', 'start_offset': 13, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'light', 'start_offset': 26, 'end_offset': 31, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'room', 'start_offset': 39, 'end_offset': 43, 'type': '<ALPHANUM>', 'position': 9}\n",
      "{'token': 'read', 'start_offset': 52, 'end_offset': 59, 'type': '<ALPHANUM>', 'position': 12}\n",
      "{'token': 'lamp', 'start_offset': 60, 'end_offset': 64, 'type': '<ALPHANUM>', 'position': 13}\n",
      "{'token': 'shade', 'start_offset': 70, 'end_offset': 75, 'type': '<ALPHANUM>', 'position': 15}\n",
      "{'token': 'screw', 'start_offset': 80, 'end_offset': 87, 'type': '<ALPHANUM>', 'position': 17}\n",
      "{'token': 'down', 'start_offset': 88, 'end_offset': 92, 'type': '<ALPHANUM>', 'position': 18}\n",
      "{'token': 'so', 'start_offset': 93, 'end_offset': 95, 'type': '<ALPHANUM>', 'position': 19}\n",
      "{'token': 'over', 'start_offset': 102, 'end_offset': 106, 'type': '<ALPHANUM>', 'position': 22}\n",
      "{'token': 'shadow', 'start_offset': 107, 'end_offset': 113, 'type': '<ALPHANUM>', 'position': 23}\n",
      "{'token': 'her', 'start_offset': 114, 'end_offset': 117, 'type': '<ALPHANUM>', 'position': 24}\n",
      "{'token': 'face', 'start_offset': 118, 'end_offset': 122, 'type': '<ALPHANUM>', 'position': 25}\n",
      "{'token': 'instead', 'start_offset': 124, 'end_offset': 131, 'type': '<ALPHANUM>', 'position': 26}\n",
      "{'token': 'look', 'start_offset': 135, 'end_offset': 142, 'type': '<ALPHANUM>', 'position': 28}\n",
      "{'token': 'up', 'start_offset': 143, 'end_offset': 145, 'type': '<ALPHANUM>', 'position': 29}\n",
      "{'token': 'us', 'start_offset': 149, 'end_offset': 151, 'type': '<ALPHANUM>', 'position': 31}\n",
      "{'token': 'her', 'start_offset': 155, 'end_offset': 158, 'type': '<ALPHANUM>', 'position': 33}\n",
      "{'token': 'usual', 'start_offset': 159, 'end_offset': 164, 'type': '<ALPHANUM>', 'position': 34}\n",
      "{'token': 'straightforward', 'start_offset': 165, 'end_offset': 180, 'type': '<ALPHANUM>', 'position': 35}\n",
      "{'token': 'way', 'start_offset': 181, 'end_offset': 184, 'type': '<ALPHANUM>', 'position': 36}\n",
      "{'token': 'she', 'start_offset': 186, 'end_offset': 189, 'type': '<ALPHANUM>', 'position': 37}\n",
      "{'token': 'sat', 'start_offset': 190, 'end_offset': 193, 'type': '<ALPHANUM>', 'position': 38}\n",
      "{'token': 'close', 'start_offset': 194, 'end_offset': 199, 'type': '<ALPHANUM>', 'position': 39}\n",
      "{'token': 'tabl', 'start_offset': 207, 'end_offset': 212, 'type': '<ALPHANUM>', 'position': 42}\n",
      "{'token': 'kept', 'start_offset': 218, 'end_offset': 222, 'type': '<ALPHANUM>', 'position': 44}\n",
      "{'token': 'her', 'start_offset': 223, 'end_offset': 226, 'type': '<ALPHANUM>', 'position': 45}\n",
      "{'token': 'eye', 'start_offset': 227, 'end_offset': 231, 'type': '<ALPHANUM>', 'position': 46}\n",
      "{'token': 'fix', 'start_offset': 232, 'end_offset': 237, 'type': '<ALPHANUM>', 'position': 47}\n",
      "{'token': 'obstin', 'start_offset': 238, 'end_offset': 249, 'type': '<ALPHANUM>', 'position': 48}\n",
      "{'token': 'open', 'start_offset': 256, 'end_offset': 260, 'type': '<ALPHANUM>', 'position': 51}\n",
      "{'token': 'book', 'start_offset': 261, 'end_offset': 265, 'type': '<ALPHANUM>', 'position': 52}\n",
      "{'token': 'offic', 'start_offset': 268, 'end_offset': 275, 'type': '<ALPHANUM>', 'position': 53}\n",
      "{'token': 'she', 'start_offset': 278, 'end_offset': 281, 'type': '<ALPHANUM>', 'position': 54}\n",
      "{'token': 'said', 'start_offset': 282, 'end_offset': 286, 'type': '<ALPHANUM>', 'position': 55}\n",
      "{'token': 'import', 'start_offset': 295, 'end_offset': 304, 'type': '<ALPHANUM>', 'position': 58}\n",
      "{'token': 'inquiri', 'start_offset': 312, 'end_offset': 319, 'type': '<ALPHANUM>', 'position': 61}\n",
      "{'token': 'you', 'start_offset': 320, 'end_offset': 323, 'type': '<ALPHANUM>', 'position': 62}\n",
      "{'token': 'conduct', 'start_offset': 328, 'end_offset': 338, 'type': '<ALPHANUM>', 'position': 64}\n",
      "{'token': 'know', 'start_offset': 342, 'end_offset': 346, 'type': '<ALPHANUM>', 'position': 66}\n",
      "{'token': 'beforehand', 'start_offset': 347, 'end_offset': 357, 'type': '<ALPHANUM>', 'position': 67}\n",
      "{'token': 'ani', 'start_offset': 361, 'end_offset': 364, 'type': '<ALPHANUM>', 'position': 69}\n",
      "{'token': 'person', 'start_offset': 365, 'end_offset': 371, 'type': '<ALPHANUM>', 'position': 70}\n",
      "{'token': 'now', 'start_offset': 372, 'end_offset': 375, 'type': '<ALPHANUM>', 'position': 71}\n",
      "{'token': 'hous', 'start_offset': 384, 'end_offset': 389, 'type': '<ALPHANUM>', 'position': 74}\n",
      "{'token': 'wish', 'start_offset': 390, 'end_offset': 396, 'type': '<ALPHANUM>', 'position': 75}\n",
      "{'token': 'leav', 'start_offset': 400, 'end_offset': 405, 'type': '<ALPHANUM>', 'position': 77}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', \n",
    "                        'text': moonstone})\n",
    "for i in res['tokens']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup ..\n",
    "ind.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing script `IndexFilesPreprocess.py`\n",
    "\n",
    "You should study how the provided indexer script named `IndexFilesPreprocess.py` works. \n",
    "Its usage is as follows:\n",
    "\n",
    "```\n",
    "usage: IndexFilesPreprocess.py [-h] --path PATH --index INDEX\n",
    "                               [--token {standard,whitespace,classic,letter}]\n",
    "                               [--filter ...]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --path PATH           Path to the files\n",
    "  --index INDEX         Index for the files\n",
    "  --token {standard,whitespace,classic,letter}\n",
    "                        Text tokenizer\n",
    "  --filter ...          Text filter: lowercase, asciifolding, stop,\n",
    "                        porter_stem, kstem, snowball\n",
    "```\n",
    "\n",
    "So, you can pass a `--path` argument which is the path to a directory where the files that you want to index are located (possibly in subdirectories);\n",
    "you can specify through `--index` the name of the index to be created; you can also specify the _tokenization_ procedure to be used with the `--token` argument;\n",
    "and finally you can apply preprocessing filters through the `--filter` argument. As an example call,\n",
    "\n",
    "```\n",
    "$ python3 IndexFilesPreprocess.py --index toy --path toy-docs --token letter --filter lowercase asciifolding\n",
    "```\n",
    "\n",
    "would create an index called `toy` adding all files located within the subdirectory `toy-docs`, applying the letter tokenizer and applying `lowercase` and `asciifolding` preprocessing.\n",
    "\n",
    "\n",
    "In particular, you should pay attention to:\n",
    "\n",
    "- how preprocessing is done within the script\n",
    "- how the `bulk` operation is used for adding documents to the index (instead of adding files one-by-one)\n",
    "- the structure of docuements added, which contains a `text` field with the content but also a `path` field with the name of the file being added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Suggested coding exercises\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "\n",
    "Download the `arxiv_abs.zip` repository from `https://www.cs.upc.edu/~marias/arxiv_abs.zip`; unzip it. You should see a directory containing folders that contain\n",
    "text files. These correspond to abstracts of scientific papers in several topics from the [arXiv.org](https://arxiv.org) repository. Index these abstracts using the `IndexFilesPreprocess.py` script (be patient, it takes a while). Double check that your index contains around 58K documents. Pay special attention to how file names are stored in the `path` field of the indexed elasticsearch documents.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_str = '../../../arxiv_abs/arxiv' # ../../arxiv\n",
    "index_str = 'arxiv'\n",
    "token_str = 'whitespace'\n",
    "filter_str = 'lowercase stop snowball'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 58102 files\n",
      "Reading files ...\n",
      "Index settings= {'arxiv': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'arxiv', 'creation_date': '1697316557632', 'analysis': {'analyzer': {'default': {'filter': ['lowercase', 'stop', 'snowball'], 'type': 'custom', 'tokenizer': 'whitespace'}}}, 'number_of_replicas': '1', 'uuid': 'etpwz2G7RZiq1UIN6rmhyg', 'version': {'created': '8100299'}}}}}\n",
      "Indexing ...\n"
     ]
    }
   ],
   "source": [
    "!python IndexFilesPreprocess.py --path $path_str --index $index_str --token $token_str --filter $filter_str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10000 hits\n"
     ]
    }
   ],
   "source": [
    "resp = client.search(index=\"arxiv\", query={\"match_all\": {}})\n",
    "print(f\"Got {resp['hits']['total']['value']} hits\")\n",
    "\n",
    "# delete the index\n",
    "# ind = Index(index_str, using=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../arxiv_abs/arxiv\\astro-ph.updates.on.arXiv.org/000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def generate_files_list(path):\n",
    "    for lf in os.walk(path):\n",
    "        if lf[2]:\n",
    "            for f in lf[2]:\n",
    "                print(lf[0] + '/' + f)\n",
    "                return \n",
    "  \n",
    "generate_files_list(path_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Write a function that computes the _cosine similarity_ between pairs of documents in your index. For that, you will find useful the computations from last week that computed the _tf-idf_ vectors of documents in the toy-document dataset. It is important to use _sparse representation_ for these vectors, either through the use of a python dictionary (with `term: weight` entries), or alternatively you could use a list of pairs `(term, weight)`; if you choose the latter, then it is going to be useful to sort the lists by term so that you can find common terms in order to compute the similarities.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectApiResponse({'_index': 'arxiv', '_id': 'shrzL4sBoTT17AcsVsmq', '_version': 1, '_seq_no': 0, '_primary_term': 3, 'found': True, '_source': {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000000', 'text': 'We predict the existence and observational signatures of a new class of objects that assembled early,\\nduring the first billion years of cosmic time: Obese Black-hole Galaxies (OBGs). OBGs are objects\\nin which the mass of the central black hole initially exceeds that of the stellar component of the\\nhost galaxy, and the luminosity from black-hole accretion dominates the starlight. From a cosmological\\nsimulation, we demonstrate that there are sites where star formation is initially inhibited and\\ndirect-collapse black holes (DCBHs) form due to the photo-dissociating effect of Lyman-Werner\\nradiation on molecular hydrogen. We show that the formation of OBGs is inevitable, because the probability\\nof finding the required extra-galactic environment and the right physical conditions in a halo\\nconducive to DCBH formation is quite high in the early universe. We estimate an OBG number density\\nof 0.009/Mpc^3 at z~8 and 0.03/Mpc^3 at z~6. Extrapolating from our simulation volume, we infer\\nthat the most luminous quasars detected at z~6 likely transited through an earlier OBG phase. We\\nfind that these primordial galaxies start off with an over-massive BH and acquire their stellar\\ncomponent from subsequent merging as well as in-situ star formation. In doing so, they inevitably\\ngo through an OBG phase dominated by the accretion luminosity at the Eddington rate or below, released\\nfrom the growing BH. The OBG phase is characterised by an ultra-violet (UV) spectrum with slope of\\nbeta ~ -2.3 and the absence of a Balmer Break. OBGs should also be spatially unresolved, and are expected\\nto be brighter than the majority of known high-redshift galaxies. OBGs could potentially be revealed\\nvia HST follow-up imaging of samples of brighter Lyman-break galaxies provided by wide-area ground-based\\nsurveys such as UltraVISTA, and should be easily uncovered and studied with instruments aboard\\nJWST...(abridged) '}})\n"
     ]
    }
   ],
   "source": [
    "doc1 = client.get(index=\"arxiv\", id=\"shrzL4sBoTT17AcsVsmq\")\n",
    "doc2 = client.get(index=\"arxiv\", id=\"sxrzL4sBoTT17AcsVsmq\")\n",
    "#print(doc1[\"_source\"][\"text\"])\n",
    "pprint(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_index': 'arxiv', '_id': 'shrzL4sBoTT17AcsVsmq', '_score': None, '_ignored': ['text.keyword'], '_source': {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000000', 'text': 'We predict the existence and observational signatures of a new class of objects that assembled early,\\nduring the first billion years of cosmic time: Obese Black-hole Galaxies (OBGs). OBGs are objects\\nin which the mass of the central black hole initially exceeds that of the stellar component of the\\nhost galaxy, and the luminosity from black-hole accretion dominates the starlight. From a cosmological\\nsimulation, we demonstrate that there are sites where star formation is initially inhibited and\\ndirect-collapse black holes (DCBHs) form due to the photo-dissociating effect of Lyman-Werner\\nradiation on molecular hydrogen. We show that the formation of OBGs is inevitable, because the probability\\nof finding the required extra-galactic environment and the right physical conditions in a halo\\nconducive to DCBH formation is quite high in the early universe. We estimate an OBG number density\\nof 0.009/Mpc^3 at z~8 and 0.03/Mpc^3 at z~6. Extrapolating from our simulation volume, we infer\\nthat the most luminous quasars detected at z~6 likely transited through an earlier OBG phase. We\\nfind that these primordial galaxies start off with an over-massive BH and acquire their stellar\\ncomponent from subsequent merging as well as in-situ star formation. In doing so, they inevitably\\ngo through an OBG phase dominated by the accretion luminosity at the Eddington rate or below, released\\nfrom the growing BH. The OBG phase is characterised by an ultra-violet (UV) spectrum with slope of\\nbeta ~ -2.3 and the absence of a Balmer Break. OBGs should also be spatially unresolved, and are expected\\nto be brighter than the majority of known high-redshift galaxies. OBGs could potentially be revealed\\nvia HST follow-up imaging of samples of brighter Lyman-break galaxies provided by wide-area ground-based\\nsurveys such as UltraVISTA, and should be easily uncovered and studied with instruments aboard\\nJWST...(abridged) '}, 'sort': [0]}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "sc = scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}})\n",
    "for s in sc:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_simialarity(id1: str, id2: str, index: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two documents given their ids and the index name. \n",
    "    \"\"\"\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    tv1 = client.termvectors(index=\"arxiv\", id=id1, fields=['text'], term_statistics=True, positions=False)\n",
    "    tv2 = client.termvectors(index=\"arxiv\", id=id2, fields=['text'], term_statistics=True, positions=False)\n",
    "    tv = [tv1, tv2]\n",
    "\n",
    "    resp = client.search(index=index, query={\"match_all\": {}})\n",
    "    n_docs = resp['hits']['total']['value']\n",
    "    \n",
    "    w1 = w2 = {}\n",
    "\n",
    "    for i, doc in enumerate(tv):\n",
    "        max_freq = max(doc['term_vectors']['text']['terms'][t]['term_freq'] for t in doc['term_vectors']['text']['terms'])\n",
    "        \n",
    "        for t in doc['term_vectors']['text']['terms']:\n",
    "\n",
    "            term_freq = doc['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            tf = term_freq/max_freq\n",
    "            doc_freq = doc['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "            idf = math.log(n_docs/doc_freq, 2)\n",
    "                \n",
    "            if i == 0:\n",
    "                w1[t] = tf*idf\n",
    "            else:\n",
    "                w2[t] = tf*idf\n",
    "    \n",
    "    escalar_product = 0\n",
    "    for t in w1:\n",
    "        if t in w2:\n",
    "            escalar_product += w1[t]*w2[t]\n",
    "    \n",
    "    norm1 = math.sqrt(sum(w1[t]**2 for t in w1))\n",
    "    norm2 = math.sqrt(sum(w2[t]**2 for t in w2))\n",
    "    \n",
    "    return escalar_product/(norm1*norm2)\n",
    "\n",
    "cosine_simialarity(\"shrzL4sBoTT17AcsVsmq\", \"sxrzL4sBoTT17AcsVsmq\", \"arxiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "Finally, using your code above, build a matrix that reflects the average cosine similarities between pairs of documents in different paper abstract categories. These categories are reflected in the path names of the files, e.g. in my computer, the path name to abstract `/Users/marias/Downloads/arxiv/hep-ph.updates.on.arXiv.org/000787` corresponds to the category of `hep-ph` papers. The categories are `astro-ph, cs, hep-th, physics, cond-mat, hep-ph, math, quant-ph`, which can be extracted from path names.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following piece of code may be useful to see the content of a few random documents within an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Index: arxiv with 0 documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_docs_from_index(index_name, client, max_docs):\n",
    "\n",
    "    print(f\"===================\")\n",
    "    info = client.cat.count(index=index_name, format = \"json\")[0]\n",
    "    print(f\"Index: {index_name} with {info['count']} documents.\")\n",
    "    print()\n",
    "\n",
    "    res = client.search(index=index_name, size = max_docs, query= {'match_all' : {}})\n",
    "\n",
    "    for doc in res['hits']['hits']:\n",
    "        print (doc['_id'], doc['_source'])\n",
    "\n",
    "print_docs_from_index('arxiv', Elasticsearch(\"http://localhost:9200\", request_timeout=1000), max_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
