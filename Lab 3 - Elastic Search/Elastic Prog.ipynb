{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 3: Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn how to tell ElasticSearch to apply different tokenizers and filters to the documents, like removing stopwords or stemming the words.\n",
    "- Study how these changes affect the terms that ElasticSearch puts in the index, and how this in turn affects searches.\n",
    "- Continuing previous work, implement tf-idf scheme over a repository of scietific article abstracts, including cosine measure for document similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing with ElasticSearch\n",
    "\n",
    "One of the tasks of the previous session was to remove from the documents vocabulary all those strings that were not proper words. Obviously this is a frequent task and all these kinds of DB have standard processes that help to filter and reduce the terms that are not useful for searching.\n",
    "\n",
    "Text, before being indexed, can be subjected to a pipeline of different processes that strips it from anything that will not be useful for a specific application. In ES these preprocessing pipelines are called _Analyzers_; ES includes many choices for each preprocessing step. \n",
    "\n",
    "\n",
    "The [following picture](https://www.elastic.co/es/blog/found-text-analysis-part-1) illustrates the chaining of preprocessing steps:\n",
    "\n",
    "![](https://api.contentstack.io/v2/assets/575e4c8c3dc542cb38c08267/download?uid=blt51e787daed39eae9?uid=blt51e787daed39eae9)\n",
    "\n",
    "The first step of the pipeline is usually a process that converts _raw text_ into _tokens_. We can for example tokenize a text using blanks and punctuation signs or use a language specific analyzer that detects words in an specific language or parse HTML/XML...\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) of the ElasticSearch manual explains the different text tokenizers available.\n",
    "\n",
    "Once we have obtained tokens, we can _normalize_ the strings and/or filter out valid tokens that are not useful. For instance, strings can be transformed to lowercase so all occurrences of the same word are mapped to the same token regardless of whether they were capitalized. Also, there are words that are not semantically useful when searching such as adverbs, articles or prepositions, in this case each language will have its own standard list of words; these are usually called \"_stopwords_\". Another language-specific token normalization is stemming. The stem of a word corresponds to the common part of a word from all variants are formed by inflection or addition of suffixes or prefixes. For instance, the words \"unstoppable\", \"stops\" and \"stopping\" all derive from the stem \"stop\". The idea is that all variations of a word will be represented by the same token.\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of ElasticSearch manual will give you an idea of the possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modifying `ElasticSearch` index behavior (using Analyzers)\n",
    "\n",
    "In this section we are going to learn how to set up preprocessing with ElasticSearch. We are going to do it _inline_ so that you have a few examples and get familiar with how to set up ES analyzers. We are going to showcase the different options with the toy English phrase\n",
    "\n",
    "```\n",
    "my taylor 4ís was% &printing printed rich the.\n",
    "```\n",
    "\n",
    "which contains symbols and weird things to see what effect the different tokenizers and filtering options have. We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `whitespace` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "\n",
    "# work with dummy index called 'foo'\n",
    "ind = Index('foo', using=client)\n",
    "ind.settings(number_of_shards=1)\n",
    "try:\n",
    "    # drop if exists\n",
    "    ind.delete()\n",
    "except NotFoundError:\n",
    "    pass\n",
    "\n",
    "# create it\n",
    "ind.create()\n",
    "\n",
    "# close to update analyzer to custom `my_analyzer`    \n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was%', 'start_offset': 14, 'end_offset': 18, 'type': 'word', 'position': 3}\n",
      "{'token': '&printing', 'start_offset': 19, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the.', 'start_offset': 42, 'end_offset': 46, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body=\n",
    "                  {'analyzer':'default',\n",
    "                   'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `standard` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': '<ALPHANUM>', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': '<ALPHANUM>', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': '<ALPHANUM>', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `letter` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'ís', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'is', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop` + `snowball`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'print', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'print', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** solve exercise 1 from problem set 1 using ElasticSearch. You can use the following string.\n",
    "\n",
    "```\n",
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = Index('foo', using=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase', 'stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'we', 'start_offset': 1, 'end_offset': 3, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'found', 'start_offset': 4, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': 'my', 'start_offset': 10, 'end_offset': 12, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'ladi', 'start_offset': 13, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'light', 'start_offset': 26, 'end_offset': 31, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'room', 'start_offset': 39, 'end_offset': 43, 'type': '<ALPHANUM>', 'position': 9}\n",
      "{'token': 'read', 'start_offset': 52, 'end_offset': 59, 'type': '<ALPHANUM>', 'position': 12}\n",
      "{'token': 'lamp', 'start_offset': 60, 'end_offset': 64, 'type': '<ALPHANUM>', 'position': 13}\n",
      "{'token': 'shade', 'start_offset': 70, 'end_offset': 75, 'type': '<ALPHANUM>', 'position': 15}\n",
      "{'token': 'screw', 'start_offset': 80, 'end_offset': 87, 'type': '<ALPHANUM>', 'position': 17}\n",
      "{'token': 'down', 'start_offset': 88, 'end_offset': 92, 'type': '<ALPHANUM>', 'position': 18}\n",
      "{'token': 'so', 'start_offset': 93, 'end_offset': 95, 'type': '<ALPHANUM>', 'position': 19}\n",
      "{'token': 'over', 'start_offset': 102, 'end_offset': 106, 'type': '<ALPHANUM>', 'position': 22}\n",
      "{'token': 'shadow', 'start_offset': 107, 'end_offset': 113, 'type': '<ALPHANUM>', 'position': 23}\n",
      "{'token': 'her', 'start_offset': 114, 'end_offset': 117, 'type': '<ALPHANUM>', 'position': 24}\n",
      "{'token': 'face', 'start_offset': 118, 'end_offset': 122, 'type': '<ALPHANUM>', 'position': 25}\n",
      "{'token': 'instead', 'start_offset': 124, 'end_offset': 131, 'type': '<ALPHANUM>', 'position': 26}\n",
      "{'token': 'look', 'start_offset': 135, 'end_offset': 142, 'type': '<ALPHANUM>', 'position': 28}\n",
      "{'token': 'up', 'start_offset': 143, 'end_offset': 145, 'type': '<ALPHANUM>', 'position': 29}\n",
      "{'token': 'us', 'start_offset': 149, 'end_offset': 151, 'type': '<ALPHANUM>', 'position': 31}\n",
      "{'token': 'her', 'start_offset': 155, 'end_offset': 158, 'type': '<ALPHANUM>', 'position': 33}\n",
      "{'token': 'usual', 'start_offset': 159, 'end_offset': 164, 'type': '<ALPHANUM>', 'position': 34}\n",
      "{'token': 'straightforward', 'start_offset': 165, 'end_offset': 180, 'type': '<ALPHANUM>', 'position': 35}\n",
      "{'token': 'way', 'start_offset': 181, 'end_offset': 184, 'type': '<ALPHANUM>', 'position': 36}\n",
      "{'token': 'she', 'start_offset': 186, 'end_offset': 189, 'type': '<ALPHANUM>', 'position': 37}\n",
      "{'token': 'sat', 'start_offset': 190, 'end_offset': 193, 'type': '<ALPHANUM>', 'position': 38}\n",
      "{'token': 'close', 'start_offset': 194, 'end_offset': 199, 'type': '<ALPHANUM>', 'position': 39}\n",
      "{'token': 'tabl', 'start_offset': 207, 'end_offset': 212, 'type': '<ALPHANUM>', 'position': 42}\n",
      "{'token': 'kept', 'start_offset': 218, 'end_offset': 222, 'type': '<ALPHANUM>', 'position': 44}\n",
      "{'token': 'her', 'start_offset': 223, 'end_offset': 226, 'type': '<ALPHANUM>', 'position': 45}\n",
      "{'token': 'eye', 'start_offset': 227, 'end_offset': 231, 'type': '<ALPHANUM>', 'position': 46}\n",
      "{'token': 'fix', 'start_offset': 232, 'end_offset': 237, 'type': '<ALPHANUM>', 'position': 47}\n",
      "{'token': 'obstin', 'start_offset': 238, 'end_offset': 249, 'type': '<ALPHANUM>', 'position': 48}\n",
      "{'token': 'open', 'start_offset': 256, 'end_offset': 260, 'type': '<ALPHANUM>', 'position': 51}\n",
      "{'token': 'book', 'start_offset': 261, 'end_offset': 265, 'type': '<ALPHANUM>', 'position': 52}\n",
      "{'token': 'offic', 'start_offset': 268, 'end_offset': 275, 'type': '<ALPHANUM>', 'position': 53}\n",
      "{'token': 'she', 'start_offset': 278, 'end_offset': 281, 'type': '<ALPHANUM>', 'position': 54}\n",
      "{'token': 'said', 'start_offset': 282, 'end_offset': 286, 'type': '<ALPHANUM>', 'position': 55}\n",
      "{'token': 'import', 'start_offset': 295, 'end_offset': 304, 'type': '<ALPHANUM>', 'position': 58}\n",
      "{'token': 'inquiri', 'start_offset': 312, 'end_offset': 319, 'type': '<ALPHANUM>', 'position': 61}\n",
      "{'token': 'you', 'start_offset': 320, 'end_offset': 323, 'type': '<ALPHANUM>', 'position': 62}\n",
      "{'token': 'conduct', 'start_offset': 328, 'end_offset': 338, 'type': '<ALPHANUM>', 'position': 64}\n",
      "{'token': 'know', 'start_offset': 342, 'end_offset': 346, 'type': '<ALPHANUM>', 'position': 66}\n",
      "{'token': 'beforehand', 'start_offset': 347, 'end_offset': 357, 'type': '<ALPHANUM>', 'position': 67}\n",
      "{'token': 'ani', 'start_offset': 361, 'end_offset': 364, 'type': '<ALPHANUM>', 'position': 69}\n",
      "{'token': 'person', 'start_offset': 365, 'end_offset': 371, 'type': '<ALPHANUM>', 'position': 70}\n",
      "{'token': 'now', 'start_offset': 372, 'end_offset': 375, 'type': '<ALPHANUM>', 'position': 71}\n",
      "{'token': 'hous', 'start_offset': 384, 'end_offset': 389, 'type': '<ALPHANUM>', 'position': 74}\n",
      "{'token': 'wish', 'start_offset': 390, 'end_offset': 396, 'type': '<ALPHANUM>', 'position': 75}\n",
      "{'token': 'leav', 'start_offset': 400, 'end_offset': 405, 'type': '<ALPHANUM>', 'position': 77}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', \n",
    "                        'text': moonstone})\n",
    "for i in res['tokens']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup ..\n",
    "ind.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing script `IndexFilesPreprocess.py`\n",
    "\n",
    "You should study how the provided indexer script named `IndexFilesPreprocess.py` works. \n",
    "Its usage is as follows:\n",
    "\n",
    "```\n",
    "usage: IndexFilesPreprocess.py [-h] --path PATH --index INDEX\n",
    "                               [--token {standard,whitespace,classic,letter}]\n",
    "                               [--filter ...]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --path PATH           Path to the files\n",
    "  --index INDEX         Index for the files\n",
    "  --token {standard,whitespace,classic,letter}\n",
    "                        Text tokenizer\n",
    "  --filter ...          Text filter: lowercase, asciifolding, stop,\n",
    "                        porter_stem, kstem, snowball\n",
    "```\n",
    "\n",
    "So, you can pass a `--path` argument which is the path to a directory where the files that you want to index are located (possibly in subdirectories);\n",
    "you can specify through `--index` the name of the index to be created; you can also specify the _tokenization_ procedure to be used with the `--token` argument;\n",
    "and finally you can apply preprocessing filters through the `--filter` argument. As an example call,\n",
    "\n",
    "```\n",
    "$ python3 IndexFilesPreprocess.py --index toy --path toy-docs --token letter --filter lowercase asciifolding\n",
    "```\n",
    "\n",
    "would create an index called `toy` adding all files located within the subdirectory `toy-docs`, applying the letter tokenizer and applying `lowercase` and `asciifolding` preprocessing.\n",
    "\n",
    "\n",
    "In particular, you should pay attention to:\n",
    "\n",
    "- how preprocessing is done within the script\n",
    "- how the `bulk` operation is used for adding documents to the index (instead of adding files one-by-one)\n",
    "- the structure of docuements added, which contains a `text` field with the content but also a `path` field with the name of the file being added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Suggested coding exercises\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "\n",
    "Download the `arxiv_abs.zip` repository from `https://www.cs.upc.edu/~marias/arxiv_abs.zip`; unzip it. You should see a directory containing folders that contain\n",
    "text files. These correspond to abstracts of scientific papers in several topics from the [arXiv.org](https://arxiv.org) repository. Index these abstracts using the `IndexFilesPreprocess.py` script (be patient, it takes a while). Double check that your index contains around 58K documents. Pay special attention to how file names are stored in the `path` field of the indexed elasticsearch documents.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_str = '../../../arxiv_abs/arxiv' # ../../arxiv\n",
    "index_str = 'arxiv'\n",
    "token_str = 'whitespace'\n",
    "filter_str = 'lowercase stop snowball'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 58102 files\n",
      "Reading files ...\n",
      "Index settings= {'arxiv': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'arxiv', 'creation_date': '1697316557632', 'analysis': {'analyzer': {'default': {'filter': ['lowercase', 'stop', 'snowball'], 'type': 'custom', 'tokenizer': 'whitespace'}}}, 'number_of_replicas': '1', 'uuid': 'etpwz2G7RZiq1UIN6rmhyg', 'version': {'created': '8100299'}}}}}\n",
      "Indexing ...\n"
     ]
    }
   ],
   "source": [
    "!python IndexFilesPreprocess.py --path $path_str --index $index_str --token $token_str --filter $filter_str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10000 hits\n"
     ]
    }
   ],
   "source": [
    "resp = client.search(index=\"arxiv\", query={\"match_all\": {}})\n",
    "print(f\"Got {resp['hits']['total']['value']} hits\")\n",
    "\n",
    "# delete the index\n",
    "# ind = Index(index_str, using=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../arxiv_abs/arxiv\\astro-ph.updates.on.arXiv.org/000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def generate_files_list(path):\n",
    "    for lf in os.walk(path):\n",
    "        if lf[2]:\n",
    "            for f in lf[2]:\n",
    "                print(lf[0] + '/' + f)\n",
    "                return \n",
    "  \n",
    "generate_files_list(path_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Write a function that computes the _cosine similarity_ between pairs of documents in your index. For that, you will find useful the computations from last week that computed the _tf-idf_ vectors of documents in the toy-document dataset. It is important to use _sparse representation_ for these vectors, either through the use of a python dictionary (with `term: weight` entries), or alternatively you could use a list of pairs `(term, weight)`; if you choose the latter, then it is going to be useful to sort the lists by term so that you can find common terms in order to compute the similarities.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectApiResponse({'_index': 'arxiv', '_id': 'shrzL4sBoTT17AcsVsmq', '_version': 1, '_seq_no': 0, '_primary_term': 3, 'found': True, '_source': {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000000', 'text': 'We predict the existence and observational signatures of a new class of objects that assembled early,\\nduring the first billion years of cosmic time: Obese Black-hole Galaxies (OBGs). OBGs are objects\\nin which the mass of the central black hole initially exceeds that of the stellar component of the\\nhost galaxy, and the luminosity from black-hole accretion dominates the starlight. From a cosmological\\nsimulation, we demonstrate that there are sites where star formation is initially inhibited and\\ndirect-collapse black holes (DCBHs) form due to the photo-dissociating effect of Lyman-Werner\\nradiation on molecular hydrogen. We show that the formation of OBGs is inevitable, because the probability\\nof finding the required extra-galactic environment and the right physical conditions in a halo\\nconducive to DCBH formation is quite high in the early universe. We estimate an OBG number density\\nof 0.009/Mpc^3 at z~8 and 0.03/Mpc^3 at z~6. Extrapolating from our simulation volume, we infer\\nthat the most luminous quasars detected at z~6 likely transited through an earlier OBG phase. We\\nfind that these primordial galaxies start off with an over-massive BH and acquire their stellar\\ncomponent from subsequent merging as well as in-situ star formation. In doing so, they inevitably\\ngo through an OBG phase dominated by the accretion luminosity at the Eddington rate or below, released\\nfrom the growing BH. The OBG phase is characterised by an ultra-violet (UV) spectrum with slope of\\nbeta ~ -2.3 and the absence of a Balmer Break. OBGs should also be spatially unresolved, and are expected\\nto be brighter than the majority of known high-redshift galaxies. OBGs could potentially be revealed\\nvia HST follow-up imaging of samples of brighter Lyman-break galaxies provided by wide-area ground-based\\nsurveys such as UltraVISTA, and should be easily uncovered and studied with instruments aboard\\nJWST...(abridged) '}})\n"
     ]
    }
   ],
   "source": [
    "doc1 = client.get(index=\"arxiv\", id=\"shrzL4sBoTT17AcsVsmq\")\n",
    "doc2 = client.get(index=\"arxiv\", id=\"sxrzL4sBoTT17AcsVsmq\")\n",
    "#print(doc1[\"_source\"][\"text\"])\n",
    "pprint(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shrzL4sBoTT17AcsVsmq\n",
      "sxrzL4sBoTT17AcsVsmq\n",
      "tBrzL4sBoTT17AcsVsmq\n",
      "tRrzL4sBoTT17AcsVsmq\n",
      "thrzL4sBoTT17AcsVsmq\n",
      "txrzL4sBoTT17AcsVsmq\n",
      "uBrzL4sBoTT17AcsVsmq\n",
      "uRrzL4sBoTT17AcsVsmq\n",
      "uhrzL4sBoTT17AcsVsmq\n",
      "uxrzL4sBoTT17AcsVsmq\n",
      "vBrzL4sBoTT17AcsVsmq\n",
      "vRrzL4sBoTT17AcsVsmq\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "sc = scan(client, index='arxiv', query={\"query\" : {\"match_all\": {}}})\n",
    "for i,s in enumerate(sc):\n",
    "    print(s[\"_id\"])\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02070061232018249"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_simialarity(id1: str, id2: str, index: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two documents given their ids and the index name. \n",
    "    \"\"\"\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    tv1 = client.termvectors(index=\"arxiv\", id=id1, fields=['text'], term_statistics=True, positions=False)\n",
    "    tv2 = client.termvectors(index=\"arxiv\", id=id2, fields=['text'], term_statistics=True, positions=False)\n",
    "    tv = [tv1, tv2]\n",
    "\n",
    "    resp = client.search(index=index, query={\"match_all\": {}})\n",
    "    n_docs = resp['hits']['total']['value']\n",
    "    \n",
    "    w1 = {}\n",
    "    w2 = {}\n",
    "\n",
    "    for i, doc in enumerate(tv):\n",
    "        max_freq = max(doc['term_vectors']['text']['terms'][t]['term_freq'] for t in doc['term_vectors']['text']['terms'])\n",
    "        \n",
    "        for t in doc['term_vectors']['text']['terms']:\n",
    "\n",
    "            term_freq = doc['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            tf = term_freq/max_freq\n",
    "            doc_freq = doc['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "            idf = math.log(n_docs/doc_freq, 2)\n",
    "                \n",
    "            if i == 0:\n",
    "                w1[t] = tf*idf\n",
    "            else:\n",
    "                w2[t] = tf*idf\n",
    "\n",
    "    escalar_product = 0\n",
    "    for t in w1:\n",
    "        if t in w2:\n",
    "            escalar_product += w1[t]*w2[t]\n",
    "    \n",
    "    norm1 = math.sqrt(sum(w1[t]**2 for t in w1))\n",
    "    norm2 = math.sqrt(sum(w2[t]**2 for t in w2))\n",
    "    \n",
    "    return escalar_product/(norm1*norm2)\n",
    "\n",
    "cosine_simialarity(\"sxrzL4sBoTT17AcsVsmq\",\"shrzL4sBoTT17AcsVsmq\", \"arxiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "Finally, using your code above, build a matrix that reflects the average cosine similarities between pairs of documents in different paper abstract categories. These categories are reflected in the path names of the files, e.g. in my computer, the path name to abstract `/Users/marias/Downloads/arxiv/hep-ph.updates.on.arXiv.org/000787` corresponds to the category of `hep-ph` papers. The categories are `astro-ph, cs, hep-th, physics, cond-mat, hep-ph, math, quant-ph`, which can be extracted from path names.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following piece of code may be useful to see the content of a few random documents within an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Index: arxiv with 58102 documents.\n",
      "\n",
      "shrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000000', 'text': 'We predict the existence and observational signatures of a new class of objects that assembled early,\\nduring the first billion years of cosmic time: Obese Black-hole Galaxies (OBGs). OBGs are objects\\nin which the mass of the central black hole initially exceeds that of the stellar component of the\\nhost galaxy, and the luminosity from black-hole accretion dominates the starlight. From a cosmological\\nsimulation, we demonstrate that there are sites where star formation is initially inhibited and\\ndirect-collapse black holes (DCBHs) form due to the photo-dissociating effect of Lyman-Werner\\nradiation on molecular hydrogen. We show that the formation of OBGs is inevitable, because the probability\\nof finding the required extra-galactic environment and the right physical conditions in a halo\\nconducive to DCBH formation is quite high in the early universe. We estimate an OBG number density\\nof 0.009/Mpc^3 at z~8 and 0.03/Mpc^3 at z~6. Extrapolating from our simulation volume, we infer\\nthat the most luminous quasars detected at z~6 likely transited through an earlier OBG phase. We\\nfind that these primordial galaxies start off with an over-massive BH and acquire their stellar\\ncomponent from subsequent merging as well as in-situ star formation. In doing so, they inevitably\\ngo through an OBG phase dominated by the accretion luminosity at the Eddington rate or below, released\\nfrom the growing BH. The OBG phase is characterised by an ultra-violet (UV) spectrum with slope of\\nbeta ~ -2.3 and the absence of a Balmer Break. OBGs should also be spatially unresolved, and are expected\\nto be brighter than the majority of known high-redshift galaxies. OBGs could potentially be revealed\\nvia HST follow-up imaging of samples of brighter Lyman-break galaxies provided by wide-area ground-based\\nsurveys such as UltraVISTA, and should be easily uncovered and studied with instruments aboard\\nJWST...(abridged) '}\n",
      "sxrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000001', 'text': 'Walter et al. (20012) have recently identified the J=6-5, 5-4, and 2-1 CO rotational emission lines,\\nand [C_{II}] fine-structure emission line from the star-forming interstellar medium in the high-redshift\\nsubmillimeter source HDF 850.1, at z = 5.183. We employ large velocity gradient (LVG) modeling to\\nanalyze the spectra of this source assuming the [C_{II}] and CO emissions originate from (i) separate\\nunvirialized regions, (ii) separate virialized regions, (iii) uniformly mixed unvirialized\\nregion, and (iv) uniformly mixed virialized regions. We present the best fit set of parameters,\\nincluding for each case the ratio $\\\\alpha$ between the total hydrogen/helium gas mass and the CO(1-0)\\nline luminosity. We also present computations of the ratio of H_{2} mass to [C_{II}] line-luminosity\\nfor optically thin conditions, for a range of gas temperatures and densities, for direct conversion\\nof [C_{II}] line-luminosities to \"dark-H_{2}\" masses. For HDF 850.1 we find that a model in which\\nthe CO and C^{+} are uniformly mixed in gas that is shielded from UV radiation, requires a cosmic-ray\\nor X-ray ionization rate of $\\\\zeta \\\\approx$ 10^{-13} s^{-1}, plausibly consistent with the large\\nstar-formation rate ($\\\\sim$ 10^{3} M$_{\\\\odot}$ yr^{-1}) observed in this source. Enforcing the\\ncosmological constraint posed by the abundance of dark matter halos in the standard $\\\\Lambda$CDM\\ncosmology and taking into account other possible contributions to the total gas mass, we find that\\nthree of these four models are less likely at the 2$\\\\sigma$ level. We conclude that modeling HDF 850.1\\'s\\nISM as a collection of unvirialized molecular clouds with distinct CO and C^{+} layers, for which\\n$\\\\alpha$ = 0.6 M$_{\\\\odot}$ (K km s^{-1} pc^{2})^{-1} for the CO to H_{2} mass-to-luminosity ratio,\\n(similar to the standard ULIRG value), is most consistent with the $\\\\Lambda$CDM cosmology. '}\n",
      "tBrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000002', 'text': 'WASP-43b (Hellier et al.; Gillon et al.) is one of the closest-orbiting hot Jupiters, with a semimajor\\naxis a = 0.01526 +/- 0.00018 AU and a period of only 0.81 days. However, it orbits one of the coolest\\nstars with a hot Jupiter (K7V, Tstar = 4520 +/- 120 K), giving the planet a modest equilibrium temperature\\nof Teq = 1440 +/- 40 K, assuming zero Bond albedo and uniform planetary energy redistribution. This\\nhas resulted in strong signal-to-noise-ratio (S/N) observations and deep eclipses in both Warm\\nSpitzer channels (3.6 and 4.5 microns). The eclipse depths and brightness temperatures from our\\njointly fit model are 0.346 +/- 0.013% and 1684 +/- 24 K at 3.6 microns and 0.382 +/- 0.015% and 1485\\n+/- 24 K at 4.5 microns. The eclipse timings improved the estimate of the orbital period, P, by a factor\\nof three (P = 0.81347459 +/- 2.1x10-7 days) compared to Gillon et al. and put an upper limit on the eccentricity\\n(e = 0.007+0.013-0.004). We use our Spitzer eclipse depths with two previously reported ground-based\\ndata points in the J and K bands to constrain the atmospheric properties of WASP-43b. The data rule\\nout a strong thermal inversion in the dayside atmosphere of WASP-43b. Model atmospheres with no\\nthermal inversions and fiducial oxygen-rich compositions are able to explain all the available\\ndata. These data, however, are insufficient to place stringent constraints on the molecular mixing\\nratios. The data suggest low day-night energy redistribution in the planet, consistent with previous\\nstudies, with a nominal upper limit of about 35% for the fraction of energy incident on the dayside,\\nbut redistributed to the nightside. '}\n",
      "tRrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000003', 'text': 'We present HST-COS spectra of ten quasars located behind M31, selected to investigate the properties\\nof gas associated with its extended disk and high velocity clouds (HVCs). The sightlines have impact\\nparameters ranging between b= 13 kpc and 112 kpc. No absorption is detected in the four sightlines\\nbeyond b=57 kpc. Of the six remaining sightlines, all of which lie at b<32 kpc and within the N(HI)=\\n2E18 cm^{-2} boundary of the HI disk of M31, we detect low-ionization absorption at M31 velocities\\nalong four of them (three of which include MgII absorption). We also detect MgII absorption from\\nan HVC. We find that along sightlines where both are detected, the velocity location of the low-ion\\ngas tracks the peak in 21 cm emission. High-ionization absorption is detected along the three inner\\nsightlines, but not along the three outer sightlines, for which CIV data exist. As inferred from\\n21 cm emission line maps, only one sightline may have a damped Ly-alpha system. This sightline has\\nb= 17.5 kpc, and we detect both low- and high-ion absorption lines associated with it. It is the strongest\\nsingle MgII2796 absorption line that we detect, with W_0(2796)=0.63 A. The impact parameters through\\nM31 are similar to the impact parameters of galaxies identified with MgII absorbers at redshifts\\n0.1<z<1.0 in a 2011 study by Rao et al. However, the M31 MgII2796 rest equivalent width values\\nare significantly smaller. In comparison, moderate-to-strong MgII absorption from Milky Way\\ngas is detected along all ten sightlines. Thus, this study indicates that M31 does not present itself\\nas an absorbing galaxy which is typical of higher-redshift galaxies inferred to give rise to moderate-strength\\nquasar absorption lines. M31 also appears not to possess an extensive large gaseous cross section,\\nat least not along the direction of its major axis. (Abridged.) '}\n",
      "thrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000004', 'text': 'Aims: We aim to study the effects of an inhomogeneous interstellar medium (ISM) on the strength and\\nthe shape of the Lyman alpha (Lya) line in starburst galaxies. Methods: Using our 3D Monte Carlo Lya\\nradiation transfer code, we study the radiative transfer of Lya, UV and optical continuum photons\\nin homogeneous and clumpy shells of neutral hydrogen and dust surrounding a central source. Our\\nsimulations predict the Lya and continuum escape fraction, the Lya equivalent width EW(Lya), the\\nLya line profile and their dependence on the gas geometry and the main input physical parameters.\\nResults: The ISM clumpiness is found to have a strong impact on the Lya line radiative transfer, entailing\\na strong dependence of the emergent features of the Lya line (escape fraction, EW(Lya)) on the ISM\\nmorphology. Although a clumpy and dusty ISM appears more transparent to radiation (both line and\\ncontinuum) compared to an equivalent homogeneous ISM of equal dust optical depth, we find that the\\nLya photons are, in general, still more attenuated than UV continuum radiation. As a consequence,\\nthe observed Lya equivalent width (EWobs(Lya)) is lower than the intrinsic one (EWint(Lya)) for\\nnearly all clumpy ISM configurations considered. There are, however, special conditions under\\nwhich Lya photons escape more easily than the continuum, resulting in an enhanced EWobs(Lya). The\\nrequirement for this to happen is that the ISM is almost static (galactic outflows < 200 km/s),\\nextremely clumpy (with density contrasts >10^7 in HI between clumps and the interclump medium),\\nand very dusty (E(B-V) > 0.30). When these conditions are fulfilled the emergent Lya line profile\\nshows no velocity shift and little asymmetry. Given the asymmetry and velocity shifts generally\\nobserved in starburst galaxies with Lya emission, we conclude that clumping is unlikely to significantly\\nenhance their relative Lya/UV transmission. '}\n",
      "txrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000005', 'text': 'To determine the local dark matter density around the solar system is a classical problem in astronomy.\\nRecently, Garbari et al. have devised a novel method to determine the local dark matter density from\\nstellar distribution and vertical velocity dispersion profiles perpendicular to the Galactic\\nplane. Their method has the advantages of abolishing conventional approximations and using only\\na few assumptions. Their determinations, however, preferred relatively high dark matter densities.\\nThe first aim of this paper is to carefully scrutinize the their method. The second aim is to examine\\ninfluences by observational uncertainties. To examine the influences by the observational imprecision,\\nwe create mock observation data for stars being dynamical tracers based on an analytical galaxy\\nmodel and apply parametrized observational errors to the mock data. We evaluate the accuracy of\\ndetermining the dark matter density by applying the Garbari et al. method to the mock data. In addition,\\nwe estimate a sample size and observational precisions to determine the dark matter density with\\naccuracy. We find that the Garbari et al. method is capable of determining the local dark matter density\\nwith accuracy if the sample size and observational precisions are satisfactory. However, the parallax\\nand proper motion errors can cause overestimation of the dark matter density. We estimate the required\\nprecisions of the parallax measurements to be 0.1-0.3 mas. Also, we find that the line-of-sight\\nvelocity errors can cause either underestimation or overestimation of the dark matter density.\\nFrom these results, we infer that Garbari et al. would be overestimating the local dark matter density\\ndue to use of imprecise data from the Hipparcos catalog. We expect that Gaia will provide data precise\\nenough to determine the local dark matter density using the Garbari et al. method. '}\n",
      "uBrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000006', 'text': 'Swift is a multi-wavelength observatory specifically designed for gamma-ray burst (GRB) astronomy\\nthat is operational since 2004. Swift is also a very flexible multi-purpose facility that supports\\na wide range of scientific fields such as active galactic nuclei, supernovae, cataclysmic variables,\\nGalactic transients, active stars and comets. The Swift X-ray Telescope (XRT) has collected more\\nthan 150 Ms of observations in its first seven years of operations. We present the list of all the X-ray\\npoint sources detected in XRT imaging data taken in photon counting mode during the first seven years\\nof Swift operations. All these point-like sources, excluding the Gamma-Ray Bursts (GRB), will\\nbe stored in a catalog publicly available (1SWXRT). We consider all XRT observations with exposure\\ntime > 500 s taken in the period 2005-2011. Data were reduced and analyzed with standard techniques\\nand a list of detected sources for each observation was produced. A careful visual inspection was\\nperformed to remove extended, spurious and piled-up sources. Positions, count rates, fluxes and\\nthe corresponding uncertainties were computed. We have analyzed more than 35,000 XRT fields, with\\nexposures ranging between 500 s and 100 ks, for a total exposure time of 140 Ms. The catalog includes\\n~ 89,000 entries, of which almost 85,000 are not affected by pile-up and are not GRBs. Since many XRT\\nfields were observed several times, we have a total of ~36,000 distinct celestial sources. We computed\\ncount rates in three energy bands: 0.3-10 keV (Full, or F), 0.3-3 keV (Soft, or S) and 2-10 keV (Hard,\\nor H). Each entry has a detection in at least one of these bands. In particular, we detect ~ 80,000,\\n~ 70,000 and ~ 25,500$ in the F, S and H band, respectively. Count rates were converted into fluxes\\nin the 0.5-10, 0.5-2 and 2-10 keV bands. Some possible scientific uses of the catalog are also highlighted.\\n'}\n",
      "uRrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000007', 'text': 'It is well known that there is temporal relationship between coronal mass ejections (CMEs) and associated\\nflares. The duration of the acceleration phase is related to the duration of the rise phase of a flare.\\nWe investigate CMEs associated with slow long duration events (LDEs), i.e. flares with the long\\nrising phase. We determined the relationships between flares and CMEs and analyzed the CME kinematics\\nin detail. The parameters of the flares (GOES flux, duration of the rising phase) show strong correlations\\nwith the CME parameters (velocity, acceleration during main acceleration phase and duration of\\nthe CME acceleration phase). These correlations confirm the strong relation between slow LDEs\\nand CMEs. We also analyzed the relation between the parameters of the CMEs, i.e. a velocity, an acceleration\\nduring the main acceleration phase, a duration of the acceleration phase, and a height of a CME at\\nthe end of the acceleration phase. The CMEs associated with the slow LDEs are characterized by high\\nvelocity during the propagation phase, with the median equal 1423 km/s. In half of the analyzed cases,\\nthe main acceleration was low (a<300 m/s^2), which suggests that the high velocity is caused\\nby the prolongated acceleration phase (the median for the duration of the acceleration phase is\\nequal 90 minutes). The CMEs were accelerated up to several solar radii (with the median 7 Rsun), which\\nis much higher than in typical impulsive CMEs. Therefore, slow LDEs may potentially precede extremely\\nstrong geomagnetic storms. The analysis of slow LDEs and associated CMEs may give important information\\nfor developing more accurate space weather forecasts, especially for extreme events. '}\n",
      "uhrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000008', 'text': 'The distance of NGC1316, the brightest galaxy in Fornax, is an interesting test for the cosmological\\ndistance scale. First, because Fornax is the 2nd largest cluster of galaxies at <~25 Mpc after\\nVirgo and, in contrast to Virgo, has a small line-of-sight depth; and second, because NGC1316 is\\nthe galaxy with the largest number of detected SNeIa, giving the opportunity to test the consistency\\nof SNeIa distances internally and against other indicators. We measure SBF mags in NGC1316 from\\nground and space-based imaging data, providing a homogeneous set of measurements over a wide wavelength\\ninterval. The SBF, coupled with empirical and theoretical calibrations, are used to estimate the\\ndistance to the galaxy. We present the first B-band SBF measurements of NGC1316 and use them together\\nwith the optical and near-IR SBF data to analyze the properties of field stars. Our distance modulus\\nm-M=31.59 +-0.05(stat) +-0.14(sys), when placed in a consistent Cepheid distance scale, agrees\\nwith the results from other indicators. However, our result is ~17% larger than the most recent estimate\\nbased on SNeIa. Possible explanations for this disagreement are the uncertainties on internal\\nextinction, or calibration issues. Concerning the stellar population analysis, we confirm earlier\\nresults from other indicators: the field stars in NGC1316 are dominated by a solar metallicity,\\nintermediate age component. A substantial mismatch exists between B-band SBF models and data,\\na behavior that can be accounted for by an enhanced percentage of hot horizontal branch stars. Our\\nstudy of the SBF distance to NGC1316, and the comparison with distances from other indicators, raises\\nsome concern about the homogeneity between the calibrations of different indicators. If not properly\\nplaced in the same reference scale, significant differences can occur, with dramatic impact on\\nthe cosmological distance ladder. '}\n",
      "uxrzL4sBoTT17AcsVsmq {'path': '../../../arxiv_abs/arxiv\\\\astro-ph.updates.on.arXiv.org/000009', 'text': 'Asteroseismic data can be used to determine surface gravities with precisions of < 0.05 dex by\\nusing the global seismic quantities Deltanu and nu_max along with Teff and [Fe/H]. Surface gravity\\nis also one of the four stellar properties to be derived by automatic analyses for 1 billion stars\\nfrom Gaia data (workpackage GSP_Phot). We explore seismic data from MS F, G, K stars (solar-like\\nstars) observed by Kepler as a potential calibration source for methods that Gaia will use for object\\ncharacterisation (log g). We calculate log g for bright nearby stars for which radii and masses are\\nknown, and using their global seismic quantities in a grid-based method, we determine an asteroseismic\\nlog g to within 0.01 dex of the direct calculation, thus validating the accuracy of our method. We\\nfind that errors in Teff and mainly [Fe/H] can cause systematic errors of 0.02 dex. We then apply our\\nmethod to a list of 40 stars to deliver precise values of surface gravity, i.e. sigma < 0.02 dex,\\nand we find agreement with recent literature values. Finally, we explore the precision we expect\\nin a sample of 400+ Kepler stars which have their global seismic quantities measured. We find a mean\\nuncertainty (precision) on the order of <0.02 dex in log g over the full explored range 3.8 <\\nlog g < 4.6, with the mean value varying only with stellar magnitude (0.01 - 0.02 dex). We study\\nsources of systematic errors in log g and find possible biases on the order of 0.04 dex, independent\\nof log g and magnitude, which accounts for errors in the Teff and [Fe/H] measurements, as well as from\\nusing a different grid-based method. We conclude that Kepler stars provide a wealth of reliable\\ninformation that can help to calibrate methods that Gaia will use, in particular, for source characterisation\\nwith GSP_Phot where excellent precision (small uncertainties) and accuracy in log g is obtained\\nfrom seismic data. '}\n"
     ]
    }
   ],
   "source": [
    "def print_docs_from_index(index_name, client, max_docs):\n",
    "\n",
    "    print(f\"===================\")\n",
    "    info = client.cat.count(index=index_name, format = \"json\")[0]\n",
    "    print(f\"Index: {index_name} with {info['count']} documents.\")\n",
    "    print()\n",
    "\n",
    "    res = client.search(index=index_name, size = max_docs, query= {'match_all' : {}})\n",
    "\n",
    "    for doc in res['hits']['hits']:\n",
    "        print (doc['_id'], doc['_source'])\n",
    "\n",
    "print_docs_from_index('arxiv', Elasticsearch(\"http://localhost:9200\", request_timeout=1000), max_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_cosine_similarity_between_categories(categoy1: str, category2: str, ids: dict, index: str) -> float:\n",
    "    ids1 = ids[categoy1]\n",
    "    ids2 = ids[category2]\n",
    "    \n",
    "    avg_similarity = 0\n",
    "    \n",
    "    for id1 in ids1:\n",
    "        for id2 in ids2:\n",
    "            avg_similarity += cosine_simialarity(id1, id2, index)\n",
    "            \n",
    "    return avg_similarity/(len(ids1)*len(ids2))\n",
    "  \n",
    "res = client.search(index=\"arxiv\", size = 10000, query= {'match_all' : {}})\n",
    "ids = {\"astro-ph\":[], \"cs\":[], \"hep-th\":[], \"physics\":[], \"cond-mat\":[], \"hep-ph\":[], \"math\":[], \"quant-ph\":[]}\n",
    "for doc in res['hits']['hits']:\n",
    "    category = doc['_source']['path'][25:].split('.')[0]\n",
    "    ids[category].append(doc['_id'])\n",
    "\n",
    "len(ids[\"astro-ph\"])\n",
    "#average_cosine_similarity_between_categories(\"cs\", \"math\", ids, \"arxiv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
