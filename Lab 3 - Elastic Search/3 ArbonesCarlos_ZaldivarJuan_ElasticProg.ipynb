{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 3: Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn how to tell ElasticSearch to apply different tokenizers and filters to the documents, like removing stopwords or stemming the words.\n",
    "- Study how these changes affect the terms that ElasticSearch puts in the index, and how this in turn affects searches.\n",
    "- Continuing previous work, implement tf-idf scheme over a repository of scietific article abstracts, including cosine measure for document similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing with ElasticSearch\n",
    "\n",
    "One of the tasks of the previous session was to remove from the documents vocabulary all those strings that were not proper words. Obviously this is a frequent task and all these kinds of DB have standard processes that help to filter and reduce the terms that are not useful for searching.\n",
    "\n",
    "Text, before being indexed, can be subjected to a pipeline of different processes that strips it from anything that will not be useful for a specific application. In ES these preprocessing pipelines are called _Analyzers_; ES includes many choices for each preprocessing step. \n",
    "\n",
    "\n",
    "The [following picture](https://www.elastic.co/es/blog/found-text-analysis-part-1) illustrates the chaining of preprocessing steps:\n",
    "\n",
    "![](https://api.contentstack.io/v2/assets/575e4c8c3dc542cb38c08267/download?uid=blt51e787daed39eae9?uid=blt51e787daed39eae9)\n",
    "\n",
    "The first step of the pipeline is usually a process that converts _raw text_ into _tokens_. We can for example tokenize a text using blanks and punctuation signs or use a language specific analyzer that detects words in an specific language or parse HTML/XML...\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) of the ElasticSearch manual explains the different text tokenizers available.\n",
    "\n",
    "Once we have obtained tokens, we can _normalize_ the strings and/or filter out valid tokens that are not useful. For instance, strings can be transformed to lowercase so all occurrences of the same word are mapped to the same token regardless of whether they were capitalized. Also, there are words that are not semantically useful when searching such as adverbs, articles or prepositions, in this case each language will have its own standard list of words; these are usually called \"_stopwords_\". Another language-specific token normalization is stemming. The stem of a word corresponds to the common part of a word from all variants are formed by inflection or addition of suffixes or prefixes. For instance, the words \"unstoppable\", \"stops\" and \"stopping\" all derive from the stem \"stop\". The idea is that all variations of a word will be represented by the same token.\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of ElasticSearch manual will give you an idea of the possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modifying `ElasticSearch` index behavior (using Analyzers)\n",
    "\n",
    "In this section we are going to learn how to set up preprocessing with ElasticSearch. We are going to do it _inline_ so that you have a few examples and get familiar with how to set up ES analyzers. We are going to showcase the different options with the toy English phrase\n",
    "\n",
    "```\n",
    "my taylor 4ís was% &printing printed rich the.\n",
    "```\n",
    "\n",
    "which contains symbols and weird things to see what effect the different tokenizers and filtering options have. We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `whitespace` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "\n",
    "# work with dummy index called 'foo'\n",
    "ind = Index('foo', using=client)\n",
    "ind.settings(number_of_shards=1)\n",
    "try:\n",
    "    # drop if exists\n",
    "    ind.delete()\n",
    "except NotFoundError:\n",
    "    pass\n",
    "\n",
    "# create it\n",
    "ind.create()\n",
    "\n",
    "# close to update analyzer to custom `my_analyzer`    \n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was%', 'start_offset': 14, 'end_offset': 18, 'type': 'word', 'position': 3}\n",
      "{'token': '&printing', 'start_offset': 19, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the.', 'start_offset': 42, 'end_offset': 46, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body=\n",
    "                  {'analyzer':'default',\n",
    "                   'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `standard` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': '<ALPHANUM>', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': '<ALPHANUM>', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': '<ALPHANUM>', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `letter` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'ís', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'is', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop` + `snowball`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'print', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'print', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** solve exercise 1 from problem set 1 using ElasticSearch. You can use the following string.\n",
    "\n",
    "```\n",
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = Index('foo', using=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase', 'stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'we', 'start_offset': 1, 'end_offset': 3, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'found', 'start_offset': 4, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': 'my', 'start_offset': 10, 'end_offset': 12, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'ladi', 'start_offset': 13, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'light', 'start_offset': 26, 'end_offset': 31, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'room', 'start_offset': 39, 'end_offset': 43, 'type': '<ALPHANUM>', 'position': 9}\n",
      "{'token': 'read', 'start_offset': 52, 'end_offset': 59, 'type': '<ALPHANUM>', 'position': 12}\n",
      "{'token': 'lamp', 'start_offset': 60, 'end_offset': 64, 'type': '<ALPHANUM>', 'position': 13}\n",
      "{'token': 'shade', 'start_offset': 70, 'end_offset': 75, 'type': '<ALPHANUM>', 'position': 15}\n",
      "{'token': 'screw', 'start_offset': 80, 'end_offset': 87, 'type': '<ALPHANUM>', 'position': 17}\n",
      "{'token': 'down', 'start_offset': 88, 'end_offset': 92, 'type': '<ALPHANUM>', 'position': 18}\n",
      "{'token': 'so', 'start_offset': 93, 'end_offset': 95, 'type': '<ALPHANUM>', 'position': 19}\n",
      "{'token': 'over', 'start_offset': 102, 'end_offset': 106, 'type': '<ALPHANUM>', 'position': 22}\n",
      "{'token': 'shadow', 'start_offset': 107, 'end_offset': 113, 'type': '<ALPHANUM>', 'position': 23}\n",
      "{'token': 'her', 'start_offset': 114, 'end_offset': 117, 'type': '<ALPHANUM>', 'position': 24}\n",
      "{'token': 'face', 'start_offset': 118, 'end_offset': 122, 'type': '<ALPHANUM>', 'position': 25}\n",
      "{'token': 'instead', 'start_offset': 124, 'end_offset': 131, 'type': '<ALPHANUM>', 'position': 26}\n",
      "{'token': 'look', 'start_offset': 135, 'end_offset': 142, 'type': '<ALPHANUM>', 'position': 28}\n",
      "{'token': 'up', 'start_offset': 143, 'end_offset': 145, 'type': '<ALPHANUM>', 'position': 29}\n",
      "{'token': 'us', 'start_offset': 149, 'end_offset': 151, 'type': '<ALPHANUM>', 'position': 31}\n",
      "{'token': 'her', 'start_offset': 155, 'end_offset': 158, 'type': '<ALPHANUM>', 'position': 33}\n",
      "{'token': 'usual', 'start_offset': 159, 'end_offset': 164, 'type': '<ALPHANUM>', 'position': 34}\n",
      "{'token': 'straightforward', 'start_offset': 165, 'end_offset': 180, 'type': '<ALPHANUM>', 'position': 35}\n",
      "{'token': 'way', 'start_offset': 181, 'end_offset': 184, 'type': '<ALPHANUM>', 'position': 36}\n",
      "{'token': 'she', 'start_offset': 186, 'end_offset': 189, 'type': '<ALPHANUM>', 'position': 37}\n",
      "{'token': 'sat', 'start_offset': 190, 'end_offset': 193, 'type': '<ALPHANUM>', 'position': 38}\n",
      "{'token': 'close', 'start_offset': 194, 'end_offset': 199, 'type': '<ALPHANUM>', 'position': 39}\n",
      "{'token': 'tabl', 'start_offset': 207, 'end_offset': 212, 'type': '<ALPHANUM>', 'position': 42}\n",
      "{'token': 'kept', 'start_offset': 218, 'end_offset': 222, 'type': '<ALPHANUM>', 'position': 44}\n",
      "{'token': 'her', 'start_offset': 223, 'end_offset': 226, 'type': '<ALPHANUM>', 'position': 45}\n",
      "{'token': 'eye', 'start_offset': 227, 'end_offset': 231, 'type': '<ALPHANUM>', 'position': 46}\n",
      "{'token': 'fix', 'start_offset': 232, 'end_offset': 237, 'type': '<ALPHANUM>', 'position': 47}\n",
      "{'token': 'obstin', 'start_offset': 238, 'end_offset': 249, 'type': '<ALPHANUM>', 'position': 48}\n",
      "{'token': 'open', 'start_offset': 256, 'end_offset': 260, 'type': '<ALPHANUM>', 'position': 51}\n",
      "{'token': 'book', 'start_offset': 261, 'end_offset': 265, 'type': '<ALPHANUM>', 'position': 52}\n",
      "{'token': 'offic', 'start_offset': 268, 'end_offset': 275, 'type': '<ALPHANUM>', 'position': 53}\n",
      "{'token': 'she', 'start_offset': 278, 'end_offset': 281, 'type': '<ALPHANUM>', 'position': 54}\n",
      "{'token': 'said', 'start_offset': 282, 'end_offset': 286, 'type': '<ALPHANUM>', 'position': 55}\n",
      "{'token': 'import', 'start_offset': 295, 'end_offset': 304, 'type': '<ALPHANUM>', 'position': 58}\n",
      "{'token': 'inquiri', 'start_offset': 312, 'end_offset': 319, 'type': '<ALPHANUM>', 'position': 61}\n",
      "{'token': 'you', 'start_offset': 320, 'end_offset': 323, 'type': '<ALPHANUM>', 'position': 62}\n",
      "{'token': 'conduct', 'start_offset': 328, 'end_offset': 338, 'type': '<ALPHANUM>', 'position': 64}\n",
      "{'token': 'know', 'start_offset': 342, 'end_offset': 346, 'type': '<ALPHANUM>', 'position': 66}\n",
      "{'token': 'beforehand', 'start_offset': 347, 'end_offset': 357, 'type': '<ALPHANUM>', 'position': 67}\n",
      "{'token': 'ani', 'start_offset': 361, 'end_offset': 364, 'type': '<ALPHANUM>', 'position': 69}\n",
      "{'token': 'person', 'start_offset': 365, 'end_offset': 371, 'type': '<ALPHANUM>', 'position': 70}\n",
      "{'token': 'now', 'start_offset': 372, 'end_offset': 375, 'type': '<ALPHANUM>', 'position': 71}\n",
      "{'token': 'hous', 'start_offset': 384, 'end_offset': 389, 'type': '<ALPHANUM>', 'position': 74}\n",
      "{'token': 'wish', 'start_offset': 390, 'end_offset': 396, 'type': '<ALPHANUM>', 'position': 75}\n",
      "{'token': 'leav', 'start_offset': 400, 'end_offset': 405, 'type': '<ALPHANUM>', 'position': 77}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', \n",
    "                        'text': moonstone})\n",
    "for i in res['tokens']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup ..\n",
    "ind.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing script `IndexFilesPreprocess.py`\n",
    "\n",
    "You should study how the provided indexer script named `IndexFilesPreprocess.py` works. \n",
    "Its usage is as follows:\n",
    "\n",
    "```\n",
    "usage: IndexFilesPreprocess.py [-h] --path PATH --index INDEX\n",
    "                               [--token {standard,whitespace,classic,letter}]\n",
    "                               [--filter ...]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --path PATH           Path to the files\n",
    "  --index INDEX         Index for the files\n",
    "  --token {standard,whitespace,classic,letter}\n",
    "                        Text tokenizer\n",
    "  --filter ...          Text filter: lowercase, asciifolding, stop,\n",
    "                        porter_stem, kstem, snowball\n",
    "```\n",
    "\n",
    "So, you can pass a `--path` argument which is the path to a directory where the files that you want to index are located (possibly in subdirectories);\n",
    "you can specify through `--index` the name of the index to be created; you can also specify the _tokenization_ procedure to be used with the `--token` argument;\n",
    "and finally you can apply preprocessing filters through the `--filter` argument. As an example call,\n",
    "\n",
    "```\n",
    "$ python3 IndexFilesPreprocess.py --index toy --path toy-docs --token letter --filter lowercase asciifolding\n",
    "```\n",
    "\n",
    "would create an index called `toy` adding all files located within the subdirectory `toy-docs`, applying the letter tokenizer and applying `lowercase` and `asciifolding` preprocessing.\n",
    "\n",
    "\n",
    "In particular, you should pay attention to:\n",
    "\n",
    "- how preprocessing is done within the script\n",
    "- how the `bulk` operation is used for adding documents to the index (instead of adding files one-by-one)\n",
    "- the structure of docuements added, which contains a `text` field with the content but also a `path` field with the name of the file being added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Suggested coding exercises\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "\n",
    "Download the `arxiv_abs.zip` repository from `https://www.cs.upc.edu/~marias/arxiv_abs.zip`; unzip it. You should see a directory containing folders that contain\n",
    "text files. These correspond to abstracts of scientific papers in several topics from the [arXiv.org](https://arxiv.org) repository. Index these abstracts using the `IndexFilesPreprocess.py` script (be patient, it takes a while). Double check that your index contains around 58K documents. Pay special attention to how file names are stored in the `path` field of the indexed elasticsearch documents.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Write a function that computes the _cosine similarity_ between pairs of documents in your index. For that, you will find useful the computations from last week that computed the _tf-idf_ vectors of documents in the toy-document dataset. It is important to use _sparse representation_ for these vectors, either through the use of a python dictionary (with `term: weight` entries), or alternatively you could use a list of pairs `(term, weight)`; if you choose the latter, then it is going to be useful to sort the lists by term so that you can find common terms in order to compute the similarities.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "Finally, using your code above, build a matrix that reflects the average cosine similarities between pairs of documents in different paper abstract categories. These categories are reflected in the path names of the files, e.g. in my computer, the path name to abstract `/Users/marias/Downloads/arxiv/hep-ph.updates.on.arXiv.org/000787` corresponds to the category of `hep-ph` papers. The categories are `astro-ph, cs, hep-th, physics, cond-mat, hep-ph, math, quant-ph`, which can be extracted from path names.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following piece of code may be useful to see the content of a few random documents within an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "NotFoundError(404, 'index_not_found_exception', 'no such index [arxiv]', arxiv, index_or_alias)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Carlos Arbonés\\Desktop\\repositories\\Search-and-Analysis-of-Information\\Lab 3 - Elastic Search\\3 ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m res[\u001b[39m'\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mprint\u001b[39m (doc[\u001b[39m'\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m'\u001b[39m], doc[\u001b[39m'\u001b[39m\u001b[39m_source\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m print_docs_from_index(\u001b[39m'\u001b[39;49m\u001b[39marxiv\u001b[39;49m\u001b[39m'\u001b[39;49m, Elasticsearch(\u001b[39m\"\u001b[39;49m\u001b[39mhttp://localhost:9200\u001b[39;49m\u001b[39m\"\u001b[39;49m, request_timeout\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m), max_docs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Carlos Arbonés\\Desktop\\repositories\\Search-and-Analysis-of-Information\\Lab 3 - Elastic Search\\3 ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb Cell 36\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_docs_from_index\u001b[39m(index_name, client, max_docs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m===================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     info \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcat\u001b[39m.\u001b[39;49mcount(index\u001b[39m=\u001b[39;49mindex_name, \u001b[39mformat\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndex: \u001b[39m\u001b[39m{\u001b[39;00mindex_name\u001b[39m}\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m{\u001b[39;00minfo[\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m documents.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos%20Arbon%C3%A9s/Desktop/repositories/Search-and-Analysis-of-Information/Lab%203%20-%20Elastic%20Search/3%20ArbonesCarlos_ZaldivarJuan_ElasticProg.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\elasticsearch\\_sync\\client\\utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\elasticsearch\\_sync\\client\\cat.py:357\u001b[0m, in \u001b[0;36mCatClient.count\u001b[1;34m(self, index, error_trace, filter_path, format, h, help, human, local, master_timeout, pretty, s, v)\u001b[0m\n\u001b[0;32m    355\u001b[0m     __query[\u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m v\n\u001b[0;32m    356\u001b[0m __headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39maccept\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext/plain,application/json\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m--> 357\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m    358\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers\n\u001b[0;32m    359\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\elasticsearch\\_sync\\client\\_base.py:389\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[1;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\n\u001b[0;32m    379\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    380\u001b[0m     method: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[39m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[39m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mperform_request(\n\u001b[0;32m    390\u001b[0m         method, path, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders, body\u001b[39m=\u001b[39;49mbody\n\u001b[0;32m    391\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\elasticsearch\\_sync\\client\\_base.py:320\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[1;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    318\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[39m.\u001b[39mget(meta\u001b[39m.\u001b[39mstatus, ApiError)(\n\u001b[0;32m    321\u001b[0m         message\u001b[39m=\u001b[39mmessage, meta\u001b[39m=\u001b[39mmeta, body\u001b[39m=\u001b[39mresp_body\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    324\u001b[0m \u001b[39m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verified_elasticsearch:\n\u001b[0;32m    326\u001b[0m     \u001b[39m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: NotFoundError(404, 'index_not_found_exception', 'no such index [arxiv]', arxiv, index_or_alias)"
     ]
    }
   ],
   "source": [
    "def print_docs_from_index(index_name, client, max_docs):\n",
    "\n",
    "    print(f\"===================\")\n",
    "    info = client.cat.count(index=index_name, format = \"json\")[0]\n",
    "    print(f\"Index: {index_name} with {info['count']} documents.\")\n",
    "    print()\n",
    "\n",
    "    res = client.search(index=index_name, size = max_docs, query= {'match_all' : {}})\n",
    "\n",
    "    for doc in res['hits']['hits']:\n",
    "        print (doc['_id'], doc['_source'])\n",
    "\n",
    "print_docs_from_index('arxiv', Elasticsearch(\"http://localhost:9200\", request_timeout=1000), max_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
