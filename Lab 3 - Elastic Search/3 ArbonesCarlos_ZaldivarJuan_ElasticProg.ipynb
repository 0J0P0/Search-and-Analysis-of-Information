{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 3: Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn how to tell ElasticSearch to apply different tokenizers and filters to the documents, like removing stopwords or stemming the words.\n",
    "- Study how these changes affect the terms that ElasticSearch puts in the index, and how this in turn affects searches.\n",
    "- Continuing previous work, implement tf-idf scheme over a repository of scietific article abstracts, including cosine measure for document similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing with ElasticSearch\n",
    "\n",
    "One of the tasks of the previous session was to remove from the documents vocabulary all those strings that were not proper words. Obviously this is a frequent task and all these kinds of DB have standard processes that help to filter and reduce the terms that are not useful for searching.\n",
    "\n",
    "Text, before being indexed, can be subjected to a pipeline of different processes that strips it from anything that will not be useful for a specific application. In ES these preprocessing pipelines are called _Analyzers_; ES includes many choices for each preprocessing step. \n",
    "\n",
    "\n",
    "The [following picture](https://www.elastic.co/es/blog/found-text-analysis-part-1) illustrates the chaining of preprocessing steps:\n",
    "\n",
    "![](https://api.contentstack.io/v2/assets/575e4c8c3dc542cb38c08267/download?uid=blt51e787daed39eae9?uid=blt51e787daed39eae9)\n",
    "\n",
    "The first step of the pipeline is usually a process that converts _raw text_ into _tokens_. We can for example tokenize a text using blanks and punctuation signs or use a language specific analyzer that detects words in an specific language or parse HTML/XML...\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) of the ElasticSearch manual explains the different text tokenizers available.\n",
    "\n",
    "Once we have obtained tokens, we can _normalize_ the strings and/or filter out valid tokens that are not useful. For instance, strings can be transformed to lowercase so all occurrences of the same word are mapped to the same token regardless of whether they were capitalized. Also, there are words that are not semantically useful when searching such as adverbs, articles or prepositions, in this case each language will have its own standard list of words; these are usually called \"_stopwords_\". Another language-specific token normalization is stemming. The stem of a word corresponds to the common part of a word from all variants are formed by inflection or addition of suffixes or prefixes. For instance, the words \"unstoppable\", \"stops\" and \"stopping\" all derive from the stem \"stop\". The idea is that all variations of a word will be represented by the same token.\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of ElasticSearch manual will give you an idea of the possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modifying `ElasticSearch` index behavior (using Analyzers)\n",
    "\n",
    "In this section we are going to learn how to set up preprocessing with ElasticSearch. We are going to do it _inline_ so that you have a few examples and get familiar with how to set up ES analyzers. We are going to showcase the different options with the toy English phrase\n",
    "\n",
    "```\n",
    "my taylor 4ís was% &printing printed rich the.\n",
    "```\n",
    "\n",
    "which contains symbols and weird things to see what effect the different tokenizers and filtering options have. We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `whitespace` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "\n",
    "# work with dummy index called 'foo'\n",
    "ind = Index('foo', using=client)\n",
    "ind.settings(number_of_shards=1)\n",
    "try:\n",
    "    # drop if exists\n",
    "    ind.delete()\n",
    "except NotFoundError:\n",
    "    pass\n",
    "\n",
    "# create it\n",
    "ind.create()\n",
    "\n",
    "# close to update analyzer to custom `my_analyzer`    \n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was%', 'start_offset': 14, 'end_offset': 18, 'type': 'word', 'position': 3}\n",
      "{'token': '&printing', 'start_offset': 19, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the.', 'start_offset': 42, 'end_offset': 46, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body=\n",
    "                  {'analyzer':'default',\n",
    "                   'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `standard` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'ís', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `letter` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'ís', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'is', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop` + `snowball`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'print', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'print', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** solve exercise 1 from problem set 1 using ElasticSearch. You can use the following string.\n",
    "\n",
    "```\n",
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup ..\n",
    "ind.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing script `IndexFilesPreprocess.py`\n",
    "\n",
    "You should study how the provided indexer script named `IndexFilesPreprocess.py` works. \n",
    "Its usage is as follows:\n",
    "\n",
    "```\n",
    "usage: IndexFilesPreprocess.py [-h] --path PATH --index INDEX\n",
    "                               [--token {standard,whitespace,classic,letter}]\n",
    "                               [--filter ...]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --path PATH           Path to the files\n",
    "  --index INDEX         Index for the files\n",
    "  --token {standard,whitespace,classic,letter}\n",
    "                        Text tokenizer\n",
    "  --filter ...          Text filter: lowercase, asciifolding, stop,\n",
    "                        porter_stem, kstem, snowball\n",
    "```\n",
    "\n",
    "So, you can pass a `--path` argument which is the path to a directory where the files that you want to index are located (possibly in subdirectories);\n",
    "you can specify through `--index` the name of the index to be created; you can also specify the _tokenization_ procedure to be used with the `--token` argument;\n",
    "and finally you can apply preprocessing filters through the `--filter` argument. As an example call,\n",
    "\n",
    "```\n",
    "$ python3 IndexFilesPreprocess.py --index toy --path toy-docs --token letter --filter lowercase asciifolding\n",
    "```\n",
    "\n",
    "would create an index called `toy` adding all files located within the subdirectory `toy-docs`, applying the letter tokenizer and applying `lowercase` and `asciifolding` preprocessing.\n",
    "\n",
    "\n",
    "In particular, you should pay attention to:\n",
    "\n",
    "- how preprocessing is done within the script\n",
    "- how the `bulk` operation is used for adding documents to the index (instead of adding files one-by-one)\n",
    "- the structure of docuements added, which contains a `text` field with the content but also a `path` field with the name of the file being added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Suggested coding exercises\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "\n",
    "Download the `arxiv_abs.zip` repository from `https://www.cs.upc.edu/~marias/arxiv_abs.zip`; unzip it. You should see a directory containing folders that contain\n",
    "text files. These correspond to abstracts of scientific papers in several topics from the [arXiv.org](https://arxiv.org) repository. Index these abstracts using the `IndexFilesPreprocess.py` script (be patient, it takes a while). Double check that your index contains around 58K documents. Pay special attention to how file names are stored in the `path` field of the indexed elasticsearch documents.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Write a function that computes the _cosine similarity_ between pairs of documents in your index. For that, you will find useful the computations from last week that computed the _tf-idf_ vectors of documents in the toy-document dataset. It is important to use _sparse representation_ for these vectors, either through the use of a python dictionary (with `term: weight` entries), or alternatively you could use a list of pairs `(term, weight)`; if you choose the latter, then it is going to be useful to sort the lists by term so that you can find common terms in order to compute the similarities.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "Finally, using your code above, build a matrix that reflects the average cosine similarities between pairs of documents in different paper abstract categories. These categories are reflected in the path names of the files, e.g. in my computer, the path name to abstract `/Users/marias/Downloads/arxiv/hep-ph.updates.on.arXiv.org/000787` corresponds to the category of `hep-ph` papers. The categories are `astro-ph, cs, hep-th, physics, cond-mat, hep-ph, math, quant-ph`, which can be extracted from path names.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following piece of code may be useful to see the content of a few random documents within an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Index: arxiv with 58102 documents.\n",
      "\n",
      "QnRi3YoBZ6unu_CgNv5e {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000787', 'text': \"A set of eight self-consistent, time-dependent supernova (SN) simulations in three spatial dimensions\\n(3D) for 9 solar-mass and 20 solar-mass progenitors is evaluated for the presence of dipolar asymmetries\\nof the electron lepton-number emission as discovered by Tamborra et al. and termed lepton-number\\nemission self-sustained asymmetry (LESA). The simulations were performed with the Aenus-Alcar\\nneutrino/hydrodynamics code, which treats the energy- and velocity-dependent transport of neutrinos\\nof all flavors by a two-moment scheme with algebraic M1 closure. For each of the progenitors, results\\nwith fully multi-dimensional (FMD) neutrino transport and with ray-by-ray-plus (RbR+) approximation\\nare considered for two different grid resolutions. While the 9 solar-mass models develop explosions,\\nthe 20 solar-mass progenitor does not explode with the employed version of simplified neutrino\\nopacities. In all 3D models we observe the growth of substantial dipole amplitudes of the lepton-number\\n(electron neutrino minus antineutrino) flux with stable or slowly time-evolving direction and\\noverall properties fully consistent with the LESA phenomenon. Models with RbR+ transport develop\\nLESA dipoles somewhat faster and with temporarily higher amplitudes, but the FMD calculations\\nexhibit cleaner hemispheric asymmetries with a far more dominant dipole. In contrast, the RbR+\\nresults display much wider multipole spectra of the neutrino-emission anisotropies with significant\\npower also in the quadrupole and higher-order modes. Our results disprove speculations that LESA\\nis a numerical artifact of RbR+ transport. We also discuss LESA as consequence of a dipolar convection\\nflow inside of the nascent neutron star and establish, tentatively, a connection to Chandrasekhar's\\nlinear theory of thermal instability in spherical shells. \"}\n",
      "Q3Ri3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001655', 'text': 'We review the current understanding of heavy quark parton distributions in nucleons and their impact\\non deep inelastic scattering, collider physics, and other processes at high energies. The determination\\nof the heavy-quark parton distribution functions is particularly significant for the analysis\\nof hard processes at LHC energies, including the forward rapidity high $x_\\\\mathrm{F}$ domain.\\nThe contribution of \"intrinsic\" heavy quarks, which are multiply connected to the valence quarks\\nof nucleons, is reviewed within non-perturbative physics which provides new information on the\\nfundamental structure of hadrons in QCD. A new prediction for the non-perturbative intrinsic charm-anticharm\\nasymmetry of the proton eigenstate has recently been obtained from a QCD lattice gauge theory calculation\\nof the proton\\'s $G_\\\\mathrm{E}^p(Q^2)$ form factor. This form factor only arises from non-valence\\nquarks and anti-quarks if they have different contributions in the proton\\'s eigenstate. This result,\\ntogether with the exclusive and inclusive connection and analytic constraints on the form of hadronic\\nstructure functions from Light-Front Holographic QCD (LFHQCD) predicts a significant non-perturbative\\n$c(x,Q) - \\\\bar{c}(x,Q)$ asymmetry in the proton structure function at high $x$, consistent with\\nthe dynamics predicted by intrinsic charm models. Recent ATLAS data on the associated production\\nof prompt photons and charm-quark jets in $pp$ collisions at $\\\\sqrt{s} = 8$ TeV has provided new constraints\\non non-perturbative intrinsic charm and tests of the LGTH predictions. We also focus on other experimental\\nobservables which have high sensitivity to the intrinsic heavy contributions to PDFs. '}\n",
      "RHRi3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001467', 'text': 'The nature of Milky Way dwarf spheroidals (MW dSphs) has been questioned, in particular whether\\nthey are dominated by dark matter (DM). Here we investigate an alternative scenario, for which tidal\\nshocks are exerted by the MW to DM-free dSphs after a first infall of their gas-rich progenitors,\\nand for which theoretical calculations have been verified by pure N-body simulations. Whether\\nor not the dSphs are on their first infall cannot be resolved on the sole basis of their star formation\\nhistory. In fact, gas removal may cause complex gravitational instabilities and near-pericenter\\npassages can give rise to tidal disruptive processes. Advanced precision with the Gaia satellite\\nin determining both their past orbital motions and the MW velocity curve is, however, providing\\ncrucial results. First, tidal shocks explain why DM-free dSphs are found preferentially near their\\npericenter, where they are in a destructive process, while their chance to be long-lived satellites\\nis associated with a very low probability P~ 2 10^-7, which is at odds with the current DM-dominated\\ndSph scenario. Second, most dSph binding energies are consistent with a first infall. Third, the\\nMW tidal shocks that predict the observed dSph velocity dispersions are themselves predicted in\\namplitude by the most accurate MW velocity curve. Fourth, tidal shocks accurately predict the forces\\nor accelerations exerted at half-light radius of dSphs, including the MW and the Magellanic System\\ngravitational attractions. The above is suggestive of dSphs that are DM-free and tidally shocked\\nnear their pericenters, which may provoke a significant quake in our understanding of near-field\\ncosmology. '}\n",
      "RXRi3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001003', 'text': 'A novel non-minimal interaction of neutral right-handed fermion and abelian gauge field in the\\ncovariant $\\\\Theta$-exact noncommutative standard model (NCSM) which is invariant under Very\\nSpecial Relativity (VSR) Lorentz subgroup, opens an avenue to study the top quark pair production\\nat linear colliders. Here the coupling denoted as $\\\\kappa$ and the noncommutative (NC) scale $\\\\Lambda$.\\nIn this work, we consider two types of analysis, one is without considering helicity basis and another,\\nconsidering helicity states of the polarized and unpolarized initial and final particles. In the\\nfirst case, when $\\\\kappa$ is positive and for certain values of $\\\\Lambda$, we arrived a specific\\nthreshold value of machine energy (units of GeV) $\\\\sqrt{s_0}$ ($ \\\\simeq 2.52 ~\\\\Lambda + 39$ ) may\\nbe quite useful to probe NCSM with the unpolarized beam. The statistical $\\\\chi^2$ analysis of the\\nazimuthal anisotropy is quite possible when $\\\\kappa$ takes negative value $0 >\\\\kappa> -0.596$\\nwhich persuade a lower limit on NC scale $\\\\Lambda$ ($ 1.0\\\\, \\\\text{to} \\\\, 2.4\\\\,\\\\text{TeV}$) at $\\\\kappa_{max}=-0.296\\n(95\\\\%$ C.L) according to luminosity ranging from $100\\\\,fb^{-1} \\\\text{to}1000\\\\,fb^{-1}$ at machine\\nenergy $\\\\sqrt{s}=1.4\\\\,\\\\text{TeV}\\\\,\\\\text{and}\\\\, 3.0\\\\,\\\\text{TeV}$. In another case, we performed\\npolarized beam analysis to probe NCSM in the light of following observables azimuthal anisotropy,\\nhelicity correlation, and top quark helicity left-right asymmetry. The polarization of the initial\\nbeam $\\\\{ P_{e^{-}},P_{e^{+}}\\\\} = \\\\{-0.8,0.3\\\\}( \\\\{-0.8,0.6\\\\})$ enhances the ranges of lower limit\\non $\\\\Lambda$, $i.e.\\\\, 1.13 \\\\, \\\\text{to} \\\\, 2.80\\\\,\\\\text{TeV}$ at $\\\\kappa_{max}$ alongside the\\n$\\\\kappa_{max}$ enhanced into $-0.5445 \\\\,(-0.607)$ $95\\\\%$C.L accord with luminosity and $\\\\sqrt{s}$.\\nFinally, we studied the intriguing mixing of the UV and the IR by invoking $T(2)$ VSR Lorentz subgroup\\nsymmetry on NC tensor $\\\\Theta_{\\\\mu\\\\nu}$. '}\n",
      "RnRi3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001231', 'text': 'The new $\\\\gamma n\\\\to K^0\\\\Lambda$ data obtained from the CLAS and MAMI collaborations are analyzed\\nby employing an effective Lagrangian method. The constructed model can describe all available\\nexperimental data in both $\\\\gamma p \\\\to K^+\\\\Lambda$ and $\\\\gamma n\\\\to K^0\\\\Lambda$ channels, simultaneously.\\nThe background part of the model is built from the appropriate intermediate states involving the\\nnucleon, kaon, and hyperon exchanges, whereas the resonance part is constructed from the consistent\\ninteraction Lagrangians and propagators. To check the performance of the model a detailed comparison\\nbetween the calculated observables and experimental data in both isospin channels is presented,\\nfrom which a nice agreement can be observed. The discrepancy between the CLAS and MAMI data in the\\n$\\\\gamma n\\\\to K^0\\\\Lambda$ channel is analyzed by utilizing three different models; M1, M2, and M3\\nthat fit the CLAS, MAMI, and both CLAS and MAMI data sets, respectively. The effect of this discrepancy\\nis studied by investigating the significance of individual nucleon resonances and the predicted\\nbeam-target helicity asymmetry $E$ that has been measured by the CLAS collaboration recently.\\nIt is found that the $N(1720)P_{13}$, $N(1900)P_{13}$, and $N(2060)D_{15}$ resonances are significant\\nfor improving the agreement between model calculation and data. This result is relatively stable\\nto the choice of the model. The helicity asymmetry $E$ can be better explained by the models M1 and\\nM3. Finally, the effect of the $N(1680)P_{11}$ narrow resonance on the cross section of both isospin\\nchannels is explored. It is found that the effect is more sensitive in the $\\\\gamma n\\\\to K^0\\\\Lambda$\\nchannel. In this case the model M3, that fits both CLAS and MAMI data, yields a more realistic effect.\\n'}\n",
      "R3Ri3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000125', 'text': 'Soft limits of $N$-point correlation functions, in which one wavenumber is much smaller than the\\nothers, play a special role in constraining the physics of inflation. Anisotropic sources such\\nas a vector field during inflation generate distinct angular dependence in all these correlators.\\nIn this paper we focus on the four-point correlator (the trispectrum $T$). We adopt a parametrization\\nmotivated by models in which the inflaton $\\\\phi$ is coupled to a vector field through a $I^2 \\\\left(\\n\\\\phi \\\\right) F^2$ interaction, namely $T_{\\\\zeta}({\\\\bf k}_1, {\\\\bf k}_2, {\\\\bf k}_3, {\\\\bf k}_4) \\\\equiv\\n\\\\sum_n d_n [ P_n(\\\\hat{\\\\bf k}_1 \\\\cdot \\\\hat{\\\\bf k}_3) + P_n(\\\\hat{\\\\bf k}_1 \\\\cdot \\\\hat{\\\\bf k}_{12})\\n+ P_n(\\\\hat{\\\\bf k}_3 \\\\cdot \\\\hat{\\\\bf k}_{12}) ] P_{\\\\zeta}(k_1) P_{\\\\zeta}(k_3) P_\\\\zeta(k_{12})\\n+ (23~{\\\\rm perm})$, where $P_n$ denotes the Legendre polynomials. This shape is enhanced when the\\nwavenumbers of the diagonals of the quadrilateral are much smaller than the sides, ${\\\\bf k}_i$.\\nThe coefficient of the isotropic part, $d_0$, is equal to $\\\\tau_{\\\\rm NL}/6$ discussed in the literature.\\nA $I^2 \\\\left( \\\\phi \\\\right) F^2$ interaction generates $d_2 = 2 d_0$ which is, in turn, related to the\\nquadrupole modulation parameter of the power spectrum, $g_*$, as $d_2 \\\\approx 14 |g_*| N^2$ with\\n$N \\\\approx 60$. We show that $d_0$ and $d_2$ can be equally well-constrained: the expected $68 \\\\%$\\nCL error bars on these coefficients from a cosmic-variance-limited experiment measuring temperature\\nanisotropy of the cosmic microwave background up to $\\\\ell_{\\\\rm max}=2000$ are $\\\\delta d_2 \\\\approx\\n4 \\\\delta d_0 = 105$. Therefore, we can reach $|g_*|=10^{-3}$ by measuring the angle-dependent trispectrum.\\nThe current upper limit on $\\\\tau_{\\\\rm NL}$ from the ${\\\\it Planck}$ temperature maps yields $|g_*|<0.02$\\n($95 \\\\%$ CL). '}\n",
      "SHRi3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000919', 'text': \"The presence of ubiquitous magnetic fields in the universe is suggested from observations of radiation\\nand cosmic ray from galaxies or the intergalactic medium (IGM). One possible origin of cosmic magnetic\\nfields is the magnetogenesis in the primordial universe. Such magnetic fields are called primordial\\nmagnetic fields (PMFs), and are considered to affect the evolution of matter density fluctuations\\nand the thermal history of the IGM gas. Hence the information of PMFs is expected to be imprinted on\\nthe anisotropies of the cosmic microwave background (CMB) through the thermal Sunyaev-Zel'dovich\\n(tSZ) effect in the IGM. In this study, given an initial power spectrum of PMFs as $P(k)\\\\propto B_{\\\\rm\\n1Mpc}^2 k^{n_{B}}$, we calculate dynamical and thermal evolutions of the IGM under the influence\\nof PMFs, and compute the resultant angular power spectrum of the Compton $y$-parameter on the sky.\\nAs a result, we find that two physical processes driven by PMFs dominantly determine the power spectrum\\nof the Compton $y$-parameter; (i) the heating due to the ambipolar diffusion effectively works\\nto increase the temperature and the ionization fraction, and (ii) the Lorentz force drastically\\nenhances the density contrast just after the recombination epoch. These facts result in making\\nthe tSZ angular power spectrum induced by the PMFs more remarkable at $\\\\ell >10^4$ than that by galaxy\\nclusters even with $B_{\\\\rm 1Mpc}=0.1$ nG and $n_{B}=-1.0$ because the contribution from galaxy\\nclusters decreases with increasing $\\\\ell$. The measurement of the tSZ angular power spectrum on\\nhigh $\\\\ell$ modes can provide the stringent constraint on PMFs. \"}\n",
      "SXRi3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000317', 'text': \"Using the Fenchel-Eggleston theorem for convex hulls (an extension of the Caratheodory theorem),\\nwe prove that any likelihood can be maximized by either a dark matter 1- speed distribution $F(v)$\\nin Earth's frame or 2- Galactic velocity distribution $f^{\\\\rm gal}(\\\\vec{u})$, consisting of a\\nsum of delta functions. The former case applies only to time-averaged rate measurements and the\\nmaximum number of delta functions is $({\\\\mathcal N}-1)$, where ${\\\\mathcal N}$ is the total number\\nof data entries. The second case applies to any harmonic expansion coefficient of the time-dependent\\nrate and the maximum number of terms is ${\\\\mathcal N}$. Using time-averaged rates, the aforementioned\\nform of $F(v)$ results in a piecewise constant unmodulated halo function $\\\\tilde\\\\eta^0_{BF}(v_{\\\\rm\\nmin})$ (which is an integral of the speed distribution) with at most $({\\\\mathcal N}-1)$ downward\\nsteps. The authors had previously proven this result for likelihoods comprised of at least one extended\\nlikelihood, and found the best-fit halo function to be unique. This uniqueness, however, cannot\\nbe guaranteed in the more general analysis applied to arbitrary likelihoods. Thus we introduce\\na method for determining whether there exists a unique best-fit halo function, and provide a procedure\\nfor constructing either a pointwise confidence band, if the best-fit halo function is unique, or\\na degeneracy band, if it is not. Using measurements of modulation amplitudes, the aforementioned\\nform of $f^{\\\\rm gal}(\\\\vec{u})$, which is a sum of Galactic streams, yields a periodic time-dependent\\nhalo function $\\\\tilde\\\\eta_{BF}(v_{\\\\rm min}, t)$ which at any fixed time is a piecewise constant\\nfunction of $v_{\\\\rm min}$ with at most ${\\\\mathcal N}$ downward steps. In this case, we explain how\\nto construct pointwise confidence and degeneracy bands from the time-averaged halo function.\\nFinally, we show that requiring an isotropic ... \"}\n",
      "SnRi3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000773', 'text': 'We present a global fit to all data on the suppression of high energy jets and high energy hadrons in\\nthe most central heavy ion collisions at the LHC for two different collision energies, within a hybrid\\nstrong/weak coupling quenching model. Even though the measured suppression factors for hadrons\\nand jets differ significantly from one another and appear to asymptote to different values in the\\nhigh energy limit, we obtain a simultaneous description of all these data after constraining the\\nvalue of a single model parameter. We use our model to investigate the origin of the difference between\\nthe observed suppression of jets and hadrons and relate it, quantitatively, to the observed modification\\nof the jet fragmentation function in jets that have been modified by passage through the medium produced\\nin heavy ion collisions. In particular, the observed increase in the fraction of hard fragments\\nin medium-modified jets, which indicates that jets with the fewest hardest fragments lose the least\\nenergy, corresponds quantitatively to the observed difference between the suppression of hadrons\\nand jets. We argue that a harder fragmentation pattern for jets with a given energy after quenching\\nis a generic feature of any mechanism for the interaction between jets and the medium that they traverse\\nthat yields a larger suppression for wider jets. We also compare the results of our global fit to LHC\\ndata to measurements of the suppression of high energy hadrons in RHIC collisions, and find that\\nwith its parameter chosen to fit the LHC data our model is inconsistent with the RHIC data at the $3\\\\sigma$\\nlevel, suggesting that hard probes interact more strongly with the less hot quark-gluon plasma\\nproduced at RHIC. '}\n",
      "S3Ri3YoBZ6unu_CgNv5i {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000541', 'text': 'We scrutinize corrections to tribimaximal (TBM), bimaximal (BM) and democratic (DC) mixing matrices\\nfor explaining recent global fit neutrino mixing data. These corrections are parameterized in\\nterms of small orthogonal rotations (R) with corresponding modified PMNS matrices of the forms\\n\\\\big($R_{ij}^l\\\\cdot U,~U\\\\cdot R_{ij}^r,~U \\\\cdot R_{ij}^r \\\\cdot R_{kl}^r,~R_{ij}^l \\\\cdot R_{kl}^l\\n\\\\cdot U$\\\\big ) where $R_{ij}^{l, r}$ is rotation in ij sector and U is any one of these special matrices.\\nWe showed that for perturbative schemes dictated by single rotation, only \\\\big($ R_{12}^l\\\\cdot\\nU_{BM},~R_{13}^l\\\\cdot U_{BM},~U_{TBM}\\\\cdot R_{13}^r$ \\\\big ) can fit the mixing data at $3\\\\sigma$\\nlevel. However for $R_{ij}^l\\\\cdot R_{kl}^l\\\\cdot U$ type rotations, only \\\\big ($R_{23}^l\\\\cdot\\nR_{13}^l \\\\cdot U_{DC} $\\\\big ) is successful to fit all neutrino mixing angles within $1\\\\sigma$ range.\\nFor $U\\\\cdot R_{ij}^r\\\\cdot R_{kl}^r$ perturbative scheme, only \\\\big($U_{BM} \\\\cdot R_{12}^r\\\\cdot\\nR_{13}^r$,~$U_{DC} \\\\cdot R_{12}^r\\\\cdot R_{23}^r$,~$U_{TBM} \\\\cdot R_{12}^r\\\\cdot R_{13}^r$\\\\big\\n) are consistent at $1\\\\sigma$ level. The remaining double rotation cases are either excluded at\\n3$\\\\sigma$ level or successful in producing mixing angles only at $2\\\\sigma-3\\\\sigma$ level. We also\\nupdated our previous analysis on PMNS matrices of the form \\\\big($R_{ij}\\\\cdot U \\\\cdot R_{kl}$\\\\big\\n) with recent mixing data. We showed that the results modifies substantially with fitting accuracy\\nlevel decreases for all of the permitted cases except \\\\big($R_{12}\\\\cdot U_{BM}\\\\cdot R_{13}$,\\n$R_{23}\\\\cdot U_{TBM}\\\\cdot R_{13}$ and $R_{13}\\\\cdot U_{TBM} \\\\cdot R_{13}$\\\\big ) in this rotation\\nscheme. '}\n"
     ]
    }
   ],
   "source": [
    "def print_docs_from_index(index_name, client, max_docs):\n",
    "\n",
    "    print(f\"===================\")\n",
    "    info = client.cat.count(index=index_name, format = \"json\")[0]\n",
    "    print(f\"Index: {index_name} with {info['count']} documents.\")\n",
    "    print()\n",
    "\n",
    "    res = client.search(index=index_name, size = max_docs, query= {'match_all' : {}})\n",
    "\n",
    "    for doc in res['hits']['hits']:\n",
    "        print (doc['_id'], doc['_source'])\n",
    "\n",
    "print_docs_from_index('arxiv', Elasticsearch(\"http://localhost:9200\", request_timeout=1000), max_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
